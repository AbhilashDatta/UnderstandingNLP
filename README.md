# NLP-Notes:

This Repository contains Notebooks and Notes on NLP

## Tricks and Tips:

### Huggingface Trick:
1)  Download all files in Huggingface model directly 
![download image](/images/Directly%20Download.jfif)

2) Parallel Model Training:
![Parallel Model Training](/images/Parallel%20Model%20Training.jfif)

### General Tricks:

1) QUAIL Dataset: A better question answering Benchmark
![QUAIL dataset](/images/QUILDataset.jfif)

2) Stratified K Fold sampling for Multilabel by Abhishek Thakur:
![Stratified Fold](/images/stratified-fold%20for%20Multilabel%20Classification.jfif)

## Links and Blogs:

[AI Hub](https://aihub.cloud.google.com/u/0/s)

Word Embedding:
* [What is Word Embedding](https://machinelearningmastery.com/what-are-word-embeddings/)
* [Word Embeddings Transform Text Numbers](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/)
* [king man woman queen why](https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)
* [word2vec-tutorial-the-skip-gram-model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
* [word2vec-tutorial-part-2-negative-sampling/](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)

RNN Blogs:
* [sampling-strategies-for-recurrent-neural-networks](https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f)

LSTM:
* [Understanding LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [LSTM Implementation](https://mlexplained.com/2019/02/15/building-an-lstm-from-scratch-in-pytorch-lstms-in-depth-part-1/)
* [time-series-prediction-lstm-recurrent-neural-networks-python-keras](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)
* [time-series-forecasting-long-short-term-memory-network-python](https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/)
* [multivariate-time-series-forecasting-lstms-keras](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/)
* [multi-step-time-series-forecasting-long-short-term-memory-networks-python/](https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/)
* [Exploring LSTM](http://blog.echen.me/2017/05/30/exploring-lstms/)

Attention:

* [visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

Transformers:
* [illustrated-transformer](http://jalammar.github.io/illustrated-transformer/)
* [transformers-attention-in-disguise](https://www.mihaileric.com/posts/transformers-attention-in-disguise/)
* [attention](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
