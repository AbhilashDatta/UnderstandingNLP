{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CalculatingTokenForNLU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5VtEDjqBIQykhy4fFvwlE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/NLP-Notes/blob/master/CalculatingTokenForNLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok7accWodpVa",
        "colab_type": "text"
      },
      "source": [
        "Our goal is to understand, How we calculate `F1 score` for a model which generates sentence as a Output. It can help us to understand Evaluation Metrics for [Google Natural Languague competition](https://www.kaggle.com/c/tensorflow2-question-answering/overview/evaluation)\n",
        "\n",
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyUJGTNfUEoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IRb19Z6dtRe",
        "colab_type": "text"
      },
      "source": [
        "## Define Functions:\n",
        "We will use function defined in Hugging Face Library [Github Page](https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FafD3GoBcYdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spY6RtgPb1vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens(s):\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGTAdbuFT-fE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx1OI-lldwkH",
        "colab_type": "text"
      },
      "source": [
        "## Testing Function for analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXqN0ZOc12R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_gold = \"I am Bhadresh. I like to code\"\n",
        "a_pred = \"I like to Bhadresh. I am Code\""
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h6vk3ycbrnu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b2a5659-88c0-447d-c4a6-d7a39eeab9f1"
      },
      "source": [
        "compute_f1(a_gold, a_pred)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlhB7XPnbvTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 63,
      "outputs": []
    }
  ]
}