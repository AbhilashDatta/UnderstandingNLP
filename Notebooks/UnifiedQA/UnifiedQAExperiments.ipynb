{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnifiedQAExperiments.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOi533hY0kkUgP+oejSDWh3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/UnderstandingNLP/blob/master/UnifiedQAExperiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tVA61GDEytl",
        "outputId": "d144dfc0-2e82-42d6-f3fa-f25d8536cd97"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.3MB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 29.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 49.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.7MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDSeMjQtHzuY"
      },
      "source": [
        "from transformers import AutoTokenizer, T5ForConditionalGeneration"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIUH382_RFsR",
        "outputId": "b894f0b7-4cc8-49b2-8701-2df50ddd6f50"
      },
      "source": [
        "# from torch import cuda\n",
        "# device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "# print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9iU4BneVVuc"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llVmkYbOq5GA"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "def squad2_json_to_df(file):\n",
        "    f = open (file, \"r\") \n",
        "    data = json.loads(f.read())               #loading the json file.\n",
        "    iid = []                                  \n",
        "    tit = []                                  #Creating empty lists to store values.\n",
        "    con = []\n",
        "    Que = []\n",
        "    Ans_st = []\n",
        "    Txt = []\n",
        "    \n",
        "    for i in range(len(data['data'])):       #Root tag of the json file contains 'title' tag & 'paragraphs' list.\n",
        "        \n",
        "        title = data['data'][i]['title']\n",
        "        for p in range(len(data['data'][i]['paragraphs'])):  # 'paragraphs' list contains 'context' tag & 'qas' list.\n",
        "            \n",
        "            context = data['data'][i]['paragraphs'][p]['context']\n",
        "            for q in range(len(data['data'][i]['paragraphs'][p]['qas'])):  # 'qas' list contains 'question', 'Id' tag & 'answers' list.\n",
        "                \n",
        "                question = data['data'][i]['paragraphs'][p]['qas'][q]['question']\n",
        "                Id = data['data'][i]['paragraphs'][p]['qas'][q]['id']\n",
        "                for a in range(len(data['data'][i]['paragraphs'][p]['qas'][q]['answers'])): # 'answers' list contains 'ans_start', 'text' tags. \n",
        "                    \n",
        "                    ans_start = data['data'][i]['paragraphs'][p]['qas'][q]['answers'][a]['answer_start']\n",
        "                    text = data['data'][i]['paragraphs'][p]['qas'][q]['answers'][a]['text']\n",
        "                    \n",
        "                    tit.append(title)\n",
        "                    con.append(context)\n",
        "                    Que.append(question)                    # Appending values to lists\n",
        "                    iid.append(Id)\n",
        "                    Ans_st.append(ans_start)\n",
        "                    Txt.append(text)\n",
        "\n",
        "    print('Done')      # for indication perpose.\n",
        "    new_df = pd.DataFrame(columns=['Id','title','context','question','ans_start','text']) # Creating empty DataFrame.\n",
        "    new_df.Id = iid\n",
        "    new_df.title = tit           #intializing list values to the DataFrame.\n",
        "    new_df.context = con\n",
        "    new_df.question = Que\n",
        "    new_df.ans_start = Ans_st\n",
        "    new_df.text = Txt\n",
        "    print('Done')      # for indication perpose.\n",
        "    final_df = new_df.drop_duplicates(keep='first')  # Dropping duplicate rows from the create Dataframe.\n",
        "    return final_df\n",
        "\n",
        "def squad1_json_to_df(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
        "                           verbose = 1):\n",
        "    \"\"\"\n",
        "    input_file_path: path to the squad json file.\n",
        "    record_path: path to deepest level in json file default value is\n",
        "    ['data','paragraphs','qas','answers']\n",
        "    verbose: 0 to suppress it default is 1\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(input_file_path).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "#     ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "    m['context'] = idx\n",
        "#     js['q_idx'] = ndx\n",
        "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "swcYBF8ALdOv",
        "outputId": "020e03b4-4b3a-4c68-bae5-076c4bec6b6f"
      },
      "source": [
        "dev2_df = squad2_json_to_df('dev-v2.json')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ac5f07d65d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdev2_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquad2_json_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev-v2.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-95dd859c2d72>\u001b[0m in \u001b[0;36msquad2_json_to_df\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msquad2_json_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m#loading the json file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0miid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dev-v2.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsLsIM1MLgts"
      },
      "source": [
        "dev2_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oTl3PNfL2gq"
      },
      "source": [
        "dev2_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI1hud5ML9fz"
      },
      "source": [
        "def input_formatter(row):\n",
        "  paragraph=''\n",
        "  question=row['question']\n",
        "  paragraph+=row['title']\n",
        "  paragraph += row['context'].strip().replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
        "  paragraph = paragraph.replace(\"\\t\", \"\").replace(\"   \", \" \").replace(\"  \", \" \").replace(\"\\n\", \" \")\n",
        "  question = question.replace(\"\\t\", \"\").replace(\"   \", \" \").replace(\"  \", \" \").replace(\"\\n\", \" \")\n",
        "  input_qt = question.strip() + ' \\\\n '+ paragraph.strip()\n",
        "  return input_qt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhF2T9lnPuBG"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbQMz8CEVGe_"
      },
      "source": [
        "## Import Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baVT7GZUFQB0"
      },
      "source": [
        "model_name=\"allenai/unifiedqa-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model =  T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWN2M-jPppej"
      },
      "source": [
        "input = \"Which is best conductor? \\\\n (A) iron (B) feather (C) wood (D) plastic\"\n",
        "input_ids = tokenizer.encode(input, padding='max_length', max_length=384, truncation=True, return_tensors='pt')\n",
        "attention_mask = input_ids!=0\n",
        "attention_mask = attention_mask.long()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "jDYu7MxWgZVf",
        "outputId": "3c21ec33-a542-4159-8c3a-551782dd035c"
      },
      "source": [
        "dummy_output = model(input_ids, attention_mask)\n",
        "print(dummy_output)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c96bc719651c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdummy_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, head_mask, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m         )\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0merr_msg_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"decoder_\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You have to specify either {err_msg_prefix}inputs or {err_msg_prefix}inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_inputs or decoder_inputs_embeds"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjh9AtJogeLx"
      },
      "source": [
        "# Export to ONNX format\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    args=(sample_ids, sample_mask),\n",
        "    f='./distilbert_cortex_week10_cause_detection.onnx',\n",
        "    input_names=['input_ids', 'attention_mask'],\n",
        "    output_names=['output']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfd83mdAggvd"
      },
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfZgj_Q9gsZZ"
      },
      "source": [
        "sample_ids = (sample_data[\"ids\"].reshape(1, -1)).numpy()\n",
        "sample_mask = (sample_data[\"mask\"].reshape(1, -1)).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-GBpDMygs4B"
      },
      "source": [
        "outputs = ort_session.run(None, {\n",
        "    \"input_ids\": sample_ids,\n",
        "    \"attention_mask\": sample_mask\n",
        "})\n",
        "\n",
        "print(outputs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W0Tg1ql-qU3"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE8c6QK0nX3n"
      },
      "source": [
        "class Pytorch_to_Torchscript(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Pytorch_to_Torchscript, self).__init__()\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(\"allenai/unifiedqa-t5-small\").cuda()\n",
        "  def forward(self, data, attention_mask=None):\n",
        "    return self.model(data.cuda(), attention_mask,cuda())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNMbzH37-bqz"
      },
      "source": [
        "PATH = \"unifiedqa_small.pt\"\n",
        "torch.save(model, PATH)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrNGKKFhJiDc",
        "outputId": "6cfb80e0-f52c-42a3-8eab-fafb0f8f166c"
      },
      "source": [
        "# !git clone https://huggingface.co/allenai/unifiedqa-t5-base"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'unifiedqa-t5-base'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 18 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulyCVUYpFzQN"
      },
      "source": [
        "def run_model(input_string, **generator_args):\n",
        "  input_dict = tokenizer(input_string, max_length=512,  truncation=True, return_tensors=\"pt\")\n",
        "  input_ids = input_dict['input_ids'].to(device)\n",
        "  attention_mask = input_dict['attention_mask'].to(device)\n",
        "  res = model.generate(input_ids=input_ids, attention_mask=attention_mask, **generator_args)\n",
        "  res = res.to('cpu')\n",
        "  return [tokenizer.decode(x) for x in res]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrKQZV3uceJN",
        "outputId": "654e5e32-f0bb-447c-858a-42be341c6edc"
      },
      "source": [
        "run_model(\"Which is best conductor? \\\\n (A) iron (B) feather (C) wood (D) plastic\",\n",
        "         temperature=0.9, num_return_sequences=4, num_beams=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iron',\n",
              " 'iron (B) iron (C) feather (D) wood (D) plastic',\n",
              " 'iron (B) iron (C) feather (D) wood',\n",
              " 'iron (B) iron (C) feather (C) wood (D) plastic']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG-c2dQSca02",
        "outputId": "69b3730f-0bee-4e3b-f654-65fd9a8672c6"
      },
      "source": [
        "run_model(\"Scott filled a tray with juice and put it in a freezer. The next day, Scott opened the freezer. How did the juice most likely change? \\\\n (A) It condensed. (B) It evaporated. (C) It became a gas. (D) It became a solid.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It became a solid.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Do6ZNL8cYQE",
        "outputId": "42433857-c0d9-4db2-fbce-d68ff33e0a9d"
      },
      "source": [
        "run_model(\"Which is best conductor? \\\\n (A) iron (B) feather\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iron']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgkl4Rn2U5nr"
      },
      "source": [
        "## Inference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqZQdAX3OezT"
      },
      "source": [
        "dev2_df['inputs']=dev2_df.apply(input_formatter, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuH8QpRPPETm",
        "outputId": "c9c757de-4fdb-43fe-dcbb-717803b13d02"
      },
      "source": [
        "dev2_df['inputs'][:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     In what country is Normandy located? \\n Norman...\n",
              "4     When were the Normans in Normandy? \\n NormansT...\n",
              "5     When were the Normans in Normandy? \\n NormansT...\n",
              "8     From which countries did the Norse originate? ...\n",
              "12    Who was the Norse leader? \\n NormansThe Norman...\n",
              "16    What century did the Normans first gain their ...\n",
              "17    What century did the Normans first gain their ...\n",
              "18    What century did the Normans first gain their ...\n",
              "20    Who was the duke in the battle of Hastings? \\n...\n",
              "23    Who ruled the duchy of Normandy \\n NormansThe ...\n",
              "Name: inputs, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc4vRz6jOwdv",
        "outputId": "96647c0f-66e0-4325-8c6f-1de8b2c39ad7"
      },
      "source": [
        "dev2_df['prediction'] = dev2_df['inputs'].progress_apply(run_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10388/10388 [23:10<00:00,  7.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHebe93pPVpZ"
      },
      "source": [
        "\"\"\"Evaluation utilities.\"\"\"\n",
        "import argparse\n",
        "import collections\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "        return re.sub(regex, ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def make_eval_dict(exact_scores, f1_scores):\n",
        "    total = len(exact_scores)\n",
        "    return collections.OrderedDict([(\"exact\", 100.0 * sum(exact_scores.values / total)), (\"f1\", 100.0 * sum(f1_scores.values / total)),(\"total\", total)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "039JyibEjcK9",
        "outputId": "6e6f19e9-b058-48db-c4c0-b4749f0d8d64"
      },
      "source": [
        "dev2_df['text'] = dev2_df['text'].progress_apply(lambda answer: tokenizer.decode(tokenizer.encode(answer)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10388/10388 [00:03<00:00, 2717.12it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7dCYBlaUmFO",
        "outputId": "3d8e2283-1a80-4ddf-c9e3-e9adbfa2b707"
      },
      "source": [
        "dev2_df['exact_match'] = dev2_df.apply(lambda row:compute_exact(row['text'], row['prediction'][0]), axis=1)\n",
        "dev2_df['f1_score'] = dev2_df.apply(lambda row:compute_f1(row['text'], row['prediction'][0]), axis=1)\n",
        "make_eval_dict(dev2_df['exact_match'], dev2_df['f1_score'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('exact', 52.25259915286172),\n",
              "             ('f1', 72.89594377792447),\n",
              "             ('total', 10388)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "tPL5Xw_oZj_9",
        "outputId": "c2711d97-6909-490c-b364-39960318d8b1"
      },
      "source": [
        "dev2_df.query('f1_score==0')[['prediction', 'text','f1_score']].head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prediction</th>\n",
              "      <th>text</th>\n",
              "      <th>f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[Christian]</td>\n",
              "      <td>Catholic</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>[Christian]</td>\n",
              "      <td>Catholic orthodoxy</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>[nortmann]</td>\n",
              "      <td>Viking</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>[nortmann]</td>\n",
              "      <td>Norseman, Viking</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>[the river Epte]</td>\n",
              "      <td>Seine</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>[ ⁇ no answer&gt;]</td>\n",
              "      <td>Rollo</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>[norse]</td>\n",
              "      <td>Catholicism</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>[Raimbaud]</td>\n",
              "      <td>Oursel</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>[the Armenian state]</td>\n",
              "      <td>Turkish forces</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>[italo-Norman]</td>\n",
              "      <td>Norman mercenary</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              prediction                text  f1_score\n",
              "26           [Christian]            Catholic       0.0\n",
              "27           [Christian]  Catholic orthodoxy       0.0\n",
              "29            [nortmann]              Viking       0.0\n",
              "30            [nortmann]    Norseman, Viking       0.0\n",
              "41      [the river Epte]               Seine       0.0\n",
              "44       [ ⁇ no answer>]               Rollo       0.0\n",
              "47               [norse]         Catholicism       0.0\n",
              "71            [Raimbaud]              Oursel       0.0\n",
              "74  [the Armenian state]      Turkish forces       0.0\n",
              "77        [italo-Norman]    Norman mercenary       0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJKuaVm8jVoX"
      },
      "source": [
        "### Converting Model to ONNX:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WzfgNcxid-X"
      },
      "source": [
        "import os"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTWjnM-Biy0l",
        "outputId": "41b491e8-14dc-4263-d193-0a4d54ee1125"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 53180 (delta 44), reused 66 (delta 27), pack-reused 53081\u001b[K\n",
            "Receiving objects: 100% (53180/53180), 40.00 MiB | 13.43 MiB/s, done.\n",
            "Resolving deltas: 100% (37015/37015), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJsOnSLLjdbk"
      },
      "source": [
        "os.chdir('/content/transformers/src/transformers')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RFBjR8l5jkq1",
        "outputId": "6f8253d9-3e35-4230-a3ec-d6145cf23e8a"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/transformers/src/transformers'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "455sBtzaj4PS"
      },
      "source": [
        "model = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RINTNCGCi04b",
        "outputId": "3d92697d-558f-4d79-ee0e-9f697975d990"
      },
      "source": [
        "!python convert_graph_to_onnx.py --framework pt --model allenai/unifiedqa-t5-small unified_qa_small.onnx"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-25 13:23:01.142167: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\n",
            "====== Converting model to ONNX ======\n",
            "ONNX opset version set to: 11\n",
            "Loading pipeline (model: allenai/unifiedqa-t5-small, tokenizer: allenai/unifiedqa-t5-small)\n",
            "Some weights of the model checkpoint at allenai/unifiedqa-t5-small were not used when initializing T5Model: ['lm_head.weight']\n",
            "- This IS expected if you are initializing T5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Error while converting the model: Folder /content/transformers/src/transformers is not empty, aborting conversion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_ofN1uNkKEi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}