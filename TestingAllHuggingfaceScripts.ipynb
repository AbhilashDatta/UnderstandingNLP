{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestingAllHuggingfaceScripts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMnA7U48pnCdLeLhH6bYUCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/UnderstandingNLP/blob/master/TestingAllHuggingfaceScripts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYmlQiIUtS4O"
      },
      "source": [
        "## Install Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1aipZH0tEvW",
        "outputId": "ce8684ee-749e-4fbc-cdf2-08101cc5b1f1"
      },
      "source": [
        "!git clone -b val-to-eval https://github.com/bhadreshpsavani/transformers.git\n",
        "%cd transformers\n",
        "!pip install ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 70739, done.\u001b[K\n",
            "remote: Counting objects: 100% (828/828), done.\u001b[K\n",
            "remote: Compressing objects: 100% (431/431), done.\u001b[K\n",
            "remote: Total 70739 (delta 445), reused 671 (delta 336), pack-reused 69911\u001b[K\n",
            "Receiving objects: 100% (70739/70739), 53.94 MiB | 22.92 MiB/s, done.\n",
            "Resolving deltas: 100% (50015/50015), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.6.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.6.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.6.0.dev0) (2020.12.5)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.6.0.dev0-cp37-none-any.whl size=2112989 sha256=3e0ebb15bc2335d9574556742f30dce6233bf55b446c779339964a491ee41553\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-myqt70ig/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0.dev0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukrPTYieLUP",
        "outputId": "79fb588d-a1e2-4124-8d9c-06d6d77b6d73"
      },
      "source": [
        "!pip install -q -r ./examples/pytorch/language-modeling/requirements.txt\n",
        "!pip install -q -r ./examples/pytorch/multiple-choice/requirements.txt\n",
        "!pip install -q -r ./examples/pytorch/question-answering/requirements.txt\n",
        "!pip install -q -r ./examples/pytorch/summarization/requirements.txt\n",
        "!pip install -q -r ./examples/pytorch/text-classification/requirements.txt\n",
        "!pip install -q -r ./examples/pytorch/translation/requirements.txt\n",
        "!pip install -q -r ./examples/pytorch/token-classification/requirements.txt\n",
        "%cd .."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 204kB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 13.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 27.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 35.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 5.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 11.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9MB 12.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 34.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.2MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNAHYedQtXru"
      },
      "source": [
        "## Language-modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Dyh-gx5e2KX",
        "outputId": "10e4393a-3ba2-4d4f-f376-b28e3ed5c9a9"
      },
      "source": [
        "!python transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
        "--model_name_or_path gpt2 \\\n",
        "--dataset_name wikitext \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--dataset_config_name wikitext-2-raw-v1 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir /tmp/test-clm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 04:33:13.416205: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/22/2021 04:33:16 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/22/2021 04:33:16 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/test-clm, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr22_04-33-16_28090cecc987, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/test-clm, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=0, mp_parameters=)\n",
            "Downloading: 8.33kB [00:00, 5.62MB/s]       \n",
            "Downloading: 5.83kB [00:00, 3.84MB/s]       \n",
            "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.91 MiB, post-processed: Unknown size, total: 17.41 MiB) to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20...\n",
            "Downloading: 100% 4.72M/4.72M [00:00<00:00, 9.09MB/s]\n",
            "Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:33:19,079 >> https://huggingface.co/gpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4fm8zh1a\n",
            "Downloading: 100% 665/665 [00:00<00:00, 517kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:33:19,445 >> storing https://huggingface.co/gpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:33:19,446 >> creating metadata file for /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:33:19,446 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:33:19,447 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:33:19,651 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:33:19,651 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:33:19,852 >> https://huggingface.co/gpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdjuxu8kp\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 2.81MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:33:20,429 >> storing https://huggingface.co/gpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:33:20,429 >> creating metadata file for /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:33:20,631 >> https://huggingface.co/gpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4hpji77p\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.49MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:33:21,141 >> storing https://huggingface.co/gpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:33:21,141 >> creating metadata file for /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:33:21,341 >> https://huggingface.co/gpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyhl13n2r\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 3.64MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:33:21,922 >> storing https://huggingface.co/gpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:33:21,922 >> creating metadata file for /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:33:22,525 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:33:22,525 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:33:22,525 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:33:22,525 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:33:22,525 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:33:22,525 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:33:22,811 >> https://huggingface.co/gpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps8b524zd\n",
            "Downloading: 100% 548M/548M [00:37<00:00, 14.5MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:34:00,801 >> storing https://huggingface.co/gpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:34:00,801 >> creating metadata file for /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1075] 2021-04-22 04:34:00,801 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1204] 2021-04-22 04:34:06,062 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1213] 2021-04-22 04:34:06,062 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 5/5 [00:00<00:00,  7.62ba/s]\n",
            "100% 37/37 [00:05<00:00,  6.46ba/s]\n",
            "100% 4/4 [00:00<00:00,  6.80ba/s]\n",
            "100% 5/5 [00:04<00:00,  1.25ba/s]\n",
            "100% 37/37 [00:38<00:00,  1.04s/ba]\n",
            "100% 4/4 [00:03<00:00,  1.16ba/s]\n",
            "[INFO|trainer.py:1107] 2021-04-22 04:34:59,642 >> ***** Running training *****\n",
            "[INFO|trainer.py:1108] 2021-04-22 04:34:59,642 >>   Num examples = 5\n",
            "[INFO|trainer.py:1109] 2021-04-22 04:34:59,642 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1110] 2021-04-22 04:34:59,642 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1111] 2021-04-22 04:34:59,642 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1112] 2021-04-22 04:34:59,642 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1113] 2021-04-22 04:34:59,642 >>   Total optimization steps = 3\n",
            "  0% 0/3 [00:00<?, ?it/s]^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUGsmzYYfnD7",
        "outputId": "b863d0d7-0e65-4018-972d-b2ad69bd16b0"
      },
      "source": [
        "!python transformers/examples/pytorch/language-modeling/run_mlm.py \\\n",
        "--model_name_or_path distilbert-base-cased \\\n",
        "--dataset_name wikitext \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--dataset_config_name wikitext-2-raw-v1 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir /tmp/test-clm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 04:35:58.674516: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/22/2021 04:36:02 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/22/2021 04:36:02 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/test-clm, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr22_04-36-02_28090cecc987, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/test-clm, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=0, mp_parameters=)\n",
            "04/22/2021 04:36:03 - WARNING - datasets.builder -   Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:36:03,899 >> https://huggingface.co/distilbert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbu9bz30j\n",
            "Downloading: 100% 411/411 [00:00<00:00, 331kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:36:04,099 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:36:04,099 >> creating metadata file for /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:36:04,099 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:36:04,100 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:36:04,300 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:36:04,300 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:36:04,499 >> https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_qvm081_\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 873kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:36:04,945 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:36:04,945 >> creating metadata file for /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:36:05,145 >> https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgtf9tj6q\n",
            "Downloading: 100% 436k/436k [00:00<00:00, 1.43MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:36:05,655 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:36:05,655 >> creating metadata file for /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:36:06,253 >> https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpllf7t1uf\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 24.3kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:36:06,451 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:36:06,451 >> creating metadata file for /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:36:06,452 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:36:06,452 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:36:06,452 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:36:06,452 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:36:06,452 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:36:06,688 >> https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps6zgirn7\n",
            "Downloading: 100% 263M/263M [00:04<00:00, 55.0MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:36:11,760 >> storing https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:36:11,760 >> creating metadata file for /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[INFO|modeling_utils.py:1075] 2021-04-22 04:36:11,760 >> loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[INFO|modeling_utils.py:1204] 2021-04-22 04:36:14,108 >> All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:1213] 2021-04-22 04:36:14,109 >> All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
            " 60% 3/5 [00:00<00:00,  6.11ba/s][WARNING|tokenization_utils_base.py:3154] 2021-04-22 04:36:14,712 >> Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
            "100% 5/5 [00:00<00:00,  7.72ba/s]\n",
            "100% 37/37 [00:05<00:00,  6.61ba/s]\n",
            "100% 4/4 [00:00<00:00,  6.71ba/s]\n",
            "100% 5/5 [00:02<00:00,  2.17ba/s]\n",
            "100% 37/37 [00:20<00:00,  1.77ba/s]\n",
            "100% 4/4 [00:02<00:00,  1.98ba/s]\n",
            "[INFO|trainer.py:497] 2021-04-22 04:36:46,655 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1107] 2021-04-22 04:36:46,683 >> ***** Running training *****\n",
            "[INFO|trainer.py:1108] 2021-04-22 04:36:46,683 >>   Num examples = 5\n",
            "[INFO|trainer.py:1109] 2021-04-22 04:36:46,683 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1110] 2021-04-22 04:36:46,683 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1111] 2021-04-22 04:36:46,683 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1112] 2021-04-22 04:36:46,683 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1113] 2021-04-22 04:36:46,683 >>   Total optimization steps = 3\n",
            "100% 3/3 [01:00<00:00, 20.54s/it][INFO|trainer.py:1302] 2021-04-22 04:37:47,695 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 61.012, 'train_samples_per_second': 0.049, 'epoch': 3.0}\n",
            "100% 3/3 [01:00<00:00, 20.33s/it]\n",
            "[INFO|trainer.py:1768] 2021-04-22 04:37:47,823 >> Saving model checkpoint to /tmp/test-clm\n",
            "[INFO|configuration_utils.py:329] 2021-04-22 04:37:47,824 >> Configuration saved in /tmp/test-clm/config.json\n",
            "[INFO|modeling_utils.py:848] 2021-04-22 04:37:48,437 >> Model weights saved in /tmp/test-clm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1918] 2021-04-22 04:37:48,438 >> tokenizer config file saved in /tmp/test-clm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-04-22 04:37:48,438 >> Special tokens file saved in /tmp/test-clm/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:37:48,481 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:48,481 >>   epoch                      =        3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:48,481 >>   init_mem_cpu_alloc_delta   =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:48,481 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:48,481 >>   train_mem_cpu_alloc_delta  =     3976MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:48,481 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:48,481 >>   train_runtime              = 0:01:01.01\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:48,481 >>   train_samples              =          5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:48,481 >>   train_samples_per_second   =      0.049\n",
            "04/22/2021 04:37:48 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:497] 2021-04-22 04:37:48,603 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1988] 2021-04-22 04:37:48,608 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1990] 2021-04-22 04:37:48,608 >>   Num examples = 5\n",
            "[INFO|trainer.py:1993] 2021-04-22 04:37:48,608 >>   Batch size = 8\n",
            "100% 1/1 [00:00<00:00, 346.38it/s]\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:37:54,953 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:54,953 >>   epoch                     =        3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:54,954 >>   eval_loss                 =      3.083\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:54,954 >>   eval_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:54,954 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:54,954 >>   eval_runtime              = 0:00:06.22\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:54,954 >>   eval_samples              =          5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:54,954 >>   eval_samples_per_second   =      0.803\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:37:54,954 >>   perplexity                =    21.8245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqNCV_yEfqHF",
        "outputId": "f281074f-62ae-4ce0-bc5e-3c07d84be2f7"
      },
      "source": [
        "!python transformers/examples/pytorch/language-modeling/run_plm.py \\\n",
        "--model_name_or_path xlnet-base-cased \\\n",
        "--dataset_name wikitext \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--dataset_config_name wikitext-2-raw-v1 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--output_dir /tmp/test-clm \\\n",
        "--overwrite_output_dir"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 04:37:58.376474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/22/2021 04:38:00 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/22/2021 04:38:00 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/test-clm, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr22_04-38-00_28090cecc987, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/test-clm, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=0, mp_parameters=)\n",
            "04/22/2021 04:38:00 - WARNING - datasets.builder -   Reusing dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:38:00,987 >> https://huggingface.co/xlnet-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7ejdw3io\n",
            "Downloading: 100% 760/760 [00:00<00:00, 556kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:38:01,187 >> storing https://huggingface.co/xlnet-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:38:01,188 >> creating metadata file for /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:38:01,188 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:38:01,189 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:38:01,389 >> loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:38:01,390 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:38:01,590 >> https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf12rcxsd\n",
            "Downloading: 100% 798k/798k [00:00<00:00, 2.53MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:38:02,113 >> storing https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:38:02,113 >> creating metadata file for /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:38:02,316 >> https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzdw8ud6l\n",
            "Downloading: 100% 1.38M/1.38M [00:00<00:00, 3.66MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:38:02,905 >> storing https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:38:02,905 >> creating metadata file for /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:38:03,503 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:38:03,503 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:38:03,503 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:38:03,503 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:38:03,503 >> loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:38:03,769 >> https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6im4eze_\n",
            "Downloading: 100% 467M/467M [00:08<00:00, 52.6MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:38:12,935 >> storing https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:38:12,935 >> creating metadata file for /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[INFO|modeling_utils.py:1075] 2021-04-22 04:38:12,936 >> loading weights file https://huggingface.co/xlnet-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9461853998373b0b2f8ef8011a13b62a2c5f540b2c535ef3ea46ed8a062b16a9.3e214f11a50e9e03eb47535b58522fc3cc11ac67c120a9450f6276de151af987\n",
            "[INFO|modeling_utils.py:1204] 2021-04-22 04:38:17,085 >> All model checkpoint weights were used when initializing XLNetLMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1213] 2021-04-22 04:38:17,085 >> All the weights of XLNetLMHeadModel were initialized from the model checkpoint at xlnet-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLNetLMHeadModel for predictions without further training.\n",
            "100% 5/5 [00:01<00:00,  4.43ba/s]\n",
            "100% 37/37 [00:08<00:00,  4.41ba/s]\n",
            "100% 4/4 [00:00<00:00,  4.69ba/s]\n",
            "100% 5/5 [00:05<00:00,  1.18s/ba]\n",
            "100% 37/37 [00:53<00:00,  1.44s/ba]\n",
            "100% 4/4 [00:05<00:00,  1.29s/ba]\n",
            "[INFO|trainer.py:1107] 2021-04-22 04:39:32,236 >> ***** Running training *****\n",
            "[INFO|trainer.py:1108] 2021-04-22 04:39:32,236 >>   Num examples = 5\n",
            "[INFO|trainer.py:1109] 2021-04-22 04:39:32,236 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1110] 2021-04-22 04:39:32,236 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1111] 2021-04-22 04:39:32,236 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1112] 2021-04-22 04:39:32,236 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1113] 2021-04-22 04:39:32,237 >>   Total optimization steps = 3\n",
            " 33% 1/3 [01:39<03:19, 99.59s/it]^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq6UpEX2eoed"
      },
      "source": [
        "## multiple-choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovqijv9xhJ0r",
        "outputId": "d2023a7e-8728-4aa8-c60e-88f5f13dfe04"
      },
      "source": [
        "!python transformers/examples/pytorch/multiple-choice/run_swag.py \\\n",
        "--model_name_or_path distilbert-base-cased \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--learning_rate 5e-5 \\\n",
        "--num_train_epochs 3 \\\n",
        "--output_dir /tmp/swag_base \\\n",
        "--per_gpu_eval_batch_size=16 \\\n",
        "--per_device_train_batch_size=16 \\\n",
        "--overwrite_output"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 04:42:32.990021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/22/2021 04:42:37 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/22/2021 04:42:37 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/swag_base, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr22_04-42-37_28090cecc987, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/swag_base, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=0, mp_parameters=)\n",
            "Downloading: 7.97kB [00:00, 5.21MB/s]       \n",
            "Downloading: 7.10kB [00:00, 5.56MB/s]       \n",
            "Downloading and preparing dataset swag/regular (download: 41.92 MiB, generated: 44.96 MiB, post-processed: Unknown size, total: 86.88 MiB) to /root/.cache/huggingface/datasets/swag/regular/0.0.0/32c6a4e9a3c0880c9ace1df43c617cf89b7816037ebf2feabd4595ec4a02f74e...\n",
            "Downloading: 28.2MB [00:00, 77.7MB/s]\n",
            "Downloading: 7.89MB [00:00, 54.9MB/s]\n",
            "Downloading: 7.82MB [00:00, 50.2MB/s]\n",
            "Dataset swag downloaded and prepared to /root/.cache/huggingface/datasets/swag/regular/0.0.0/32c6a4e9a3c0880c9ace1df43c617cf89b7816037ebf2feabd4595ec4a02f74e. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:42:52,607 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:42:52,608 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:42:52,809 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:42:52,809 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:42:53,806 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:42:53,806 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:42:53,806 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:42:53,806 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:42:53,806 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|modeling_utils.py:1075] 2021-04-22 04:42:54,046 >> loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[WARNING|modeling_utils.py:1196] 2021-04-22 04:43:00,641 >> Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForMultipleChoice: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1207] 2021-04-22 04:43:00,641 >> Some weights of DistilBertForMultipleChoice were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 1/1 [00:00<00:00, 140.65ba/s]\n",
            "100% 1/1 [00:00<00:00, 239.26ba/s]\n",
            "[INFO|trainer.py:497] 2021-04-22 04:43:01,080 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForMultipleChoice.forward` and have been ignored: sent2, gold-source, sent1, ending3, ending1, video-id, ending0, startphrase, ending2, fold-ind.\n",
            "[INFO|trainer.py:1107] 2021-04-22 04:43:01,087 >> ***** Running training *****\n",
            "[INFO|trainer.py:1108] 2021-04-22 04:43:01,087 >>   Num examples = 5\n",
            "[INFO|trainer.py:1109] 2021-04-22 04:43:01,087 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1110] 2021-04-22 04:43:01,087 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1111] 2021-04-22 04:43:01,087 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1112] 2021-04-22 04:43:01,087 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1113] 2021-04-22 04:43:01,088 >>   Total optimization steps = 3\n",
            "[WARNING|training_args.py:651] 2021-04-22 04:43:01,102 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "100% 3/3 [00:13<00:00,  4.64s/it][INFO|trainer.py:1302] 2021-04-22 04:43:14,920 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 13.8326, 'train_samples_per_second': 0.217, 'epoch': 3.0}\n",
            "100% 3/3 [00:13<00:00,  4.60s/it]\n",
            "[INFO|trainer.py:1768] 2021-04-22 04:43:15,038 >> Saving model checkpoint to /tmp/swag_base\n",
            "[INFO|configuration_utils.py:329] 2021-04-22 04:43:15,039 >> Configuration saved in /tmp/swag_base/config.json\n",
            "[INFO|modeling_utils.py:848] 2021-04-22 04:43:15,616 >> Model weights saved in /tmp/swag_base/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1918] 2021-04-22 04:43:15,617 >> tokenizer config file saved in /tmp/swag_base/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-04-22 04:43:15,617 >> Special tokens file saved in /tmp/swag_base/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:43:15,655 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:15,655 >>   epoch                      =        3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:15,655 >>   init_mem_cpu_alloc_delta   =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:15,655 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:15,655 >>   train_mem_cpu_alloc_delta  =      860MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:15,656 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:15,656 >>   train_runtime              = 0:00:13.83\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:15,656 >>   train_samples              =          5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:15,656 >>   train_samples_per_second   =      0.217\n",
            "04/22/2021 04:43:15 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:497] 2021-04-22 04:43:15,776 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMultipleChoice.forward` and have been ignored: sent2, gold-source, sent1, ending3, ending1, video-id, ending0, startphrase, ending2, fold-ind.\n",
            "[WARNING|training_args.py:651] 2021-04-22 04:43:15,780 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:1988] 2021-04-22 04:43:15,780 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1990] 2021-04-22 04:43:15,781 >>   Num examples = 5\n",
            "[INFO|trainer.py:1993] 2021-04-22 04:43:15,781 >>   Batch size = 16\n",
            "100% 1/1 [00:00<00:00, 124.00it/s]\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:43:17,167 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:17,168 >>   epoch                     =        3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:17,168 >>   eval_accuracy             =        0.4\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:17,168 >>   eval_loss                 =     1.3856\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:17,168 >>   eval_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:17,168 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:17,168 >>   eval_runtime              = 0:00:01.26\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:17,168 >>   eval_samples              =          5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:43:17,168 >>   eval_samples_per_second   =      3.961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqMVrU4AeqKE"
      },
      "source": [
        "## question-answering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0O0easYhwTv",
        "outputId": "30369625-a45d-495b-dccd-a085723c3aca"
      },
      "source": [
        "!python transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "--model_name_or_path distilbert-base-uncased \\\n",
        "--train_file transformers/tests/fixtures/tests_samples/SQUAD/sample.json \\\n",
        "--validation_file transformers/tests/fixtures/tests_samples/SQUAD/sample.json \\\n",
        "--test_file transformers/tests/fixtures/tests_samples/SQUAD/sample.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--max_predict_samples 5 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--max_seq_length 384 \\\n",
        "--doc_stride 128 \\\n",
        "--version_2_with_negative \\\n",
        "--output_dir /tmp/debug_squad/ \\\n",
        "--overwrite_output"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 04:44:27.762965: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/22/2021 04:44:29 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/22/2021 04:44:29 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/debug_squad/, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr22_04-44-29_28090cecc987, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/debug_squad/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=0, mp_parameters=)\n",
            "04/22/2021 04:44:30 - WARNING - datasets.builder -   Using custom data configuration default-177be91cdb90c992\n",
            "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-177be91cdb90c992/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-177be91cdb90c992/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:44:30,632 >> https://huggingface.co/distilbert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplgfk7tkq\n",
            "Downloading: 100% 442/442 [00:00<00:00, 301kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:44:30,832 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:44:30,832 >> creating metadata file for /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:44:30,833 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:44:30,833 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:44:31,032 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:44:31,033 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:44:31,234 >> https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjw76zke7\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 952kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:44:31,678 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:44:31,679 >> creating metadata file for /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:44:31,880 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpeqpo4zab\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.54MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:44:32,384 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:44:32,384 >> creating metadata file for /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:44:33,013 >> https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpezpl8idu\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 23.4kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:44:33,214 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:44:33,214 >> creating metadata file for /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:44:33,215 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:44:33,215 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:44:33,215 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:44:33,215 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:44:33,215 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:44:33,450 >> https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpz1v7s0a7\n",
            "Downloading: 100% 268M/268M [00:11<00:00, 24.1MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:44:44,798 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:44:44,798 >> creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "[INFO|modeling_utils.py:1075] 2021-04-22 04:44:44,799 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "[WARNING|modeling_utils.py:1196] 2021-04-22 04:44:46,741 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1207] 2021-04-22 04:44:46,742 >> Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 1/1 [00:00<00:00, 155.01ba/s]\n",
            "100% 1/1 [00:00<00:00, 80.54ba/s]\n",
            "100% 1/1 [00:00<00:00, 82.98ba/s]\n",
            "Downloading: 6.49kB [00:00, 3.76MB/s]       \n",
            "Downloading: 11.3kB [00:00, 6.94MB/s]       \n",
            "[INFO|trainer.py:1107] 2021-04-22 04:44:48,281 >> ***** Running training *****\n",
            "[INFO|trainer.py:1108] 2021-04-22 04:44:48,281 >>   Num examples = 5\n",
            "[INFO|trainer.py:1109] 2021-04-22 04:44:48,281 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1110] 2021-04-22 04:44:48,281 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1111] 2021-04-22 04:44:48,281 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1112] 2021-04-22 04:44:48,281 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1113] 2021-04-22 04:44:48,281 >>   Total optimization steps = 3\n",
            "100% 3/3 [00:31<00:00, 10.57s/it][INFO|trainer.py:1302] 2021-04-22 04:45:19,881 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 31.5996, 'train_samples_per_second': 0.095, 'epoch': 3.0}\n",
            "100% 3/3 [00:31<00:00, 10.53s/it]\n",
            "[INFO|trainer.py:1768] 2021-04-22 04:45:20,021 >> Saving model checkpoint to /tmp/debug_squad/\n",
            "[INFO|configuration_utils.py:329] 2021-04-22 04:45:20,022 >> Configuration saved in /tmp/debug_squad/config.json\n",
            "[INFO|modeling_utils.py:848] 2021-04-22 04:45:20,627 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1918] 2021-04-22 04:45:20,627 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-04-22 04:45:20,628 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:45:20,675 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:20,675 >>   epoch                      =        3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:20,675 >>   init_mem_cpu_alloc_delta   =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:20,675 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:20,675 >>   train_mem_cpu_alloc_delta  =     2009MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:20,675 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:20,675 >>   train_runtime              = 0:00:31.59\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:20,675 >>   train_samples              =          5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:20,675 >>   train_samples_per_second   =      0.095\n",
            "04/22/2021 04:45:20 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:497] 2021-04-22 04:45:20,677 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "[INFO|trainer.py:2324] 2021-04-22 04:45:20,679 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2325] 2021-04-22 04:45:20,680 >>   Num examples = 5\n",
            "[INFO|trainer.py:2326] 2021-04-22 04:45:20,680 >>   Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer_pt_utils.py:367: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Tranformers.\n",
            "  FutureWarning,\n",
            "  0% 0/1 [00:00<?, ?it/s]04/22/2021 04:45:22 - INFO - utils_qa -   Post-processing 5 example predictions split into 5 features.\n",
            "\n",
            "100% 5/5 [00:00<00:00, 362.24it/s]\n",
            "04/22/2021 04:45:22 - INFO - utils_qa -   Saving predictions to /tmp/debug_squad/eval_predictions.json.\n",
            "04/22/2021 04:45:22 - INFO - utils_qa -   Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n",
            "04/22/2021 04:45:22 - INFO - utils_qa -   Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n",
            "100% 1/1 [00:00<00:00, 32.75it/s]\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:45:22,658 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   HasAns_exact      = 33.3333\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   HasAns_f1         = 41.6667\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   HasAns_total      =       3\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   NoAns_exact       =     0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   NoAns_f1          =     0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   NoAns_total       =       2\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   best_exact        =    60.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   best_exact_thresh =     0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   best_f1           =    65.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   best_f1_thresh    =     0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   epoch             =     3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   eval_samples      =       5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   exact             =    20.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   f1                =    25.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:45:22,659 >>   total             =       5\n",
            "04/22/2021 04:45:22 - INFO - __main__ -   *** Predict ***\n",
            "[INFO|trainer.py:497] 2021-04-22 04:45:22,660 >> The following columns in the prediction set  don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
            "[INFO|trainer.py:2324] 2021-04-22 04:45:22,662 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2325] 2021-04-22 04:45:22,663 >>   Num examples = 5\n",
            "[INFO|trainer.py:2326] 2021-04-22 04:45:22,663 >>   Batch size = 8\n",
            "  0% 0/1 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"transformers/examples/pytorch/question-answering/run_qa.py\", line 611, in <module>\n",
            "    main()\n",
            "  File \"transformers/examples/pytorch/question-answering/run_qa.py\", line 593, in main\n",
            "    results = trainer.predict(predict_dataset, predict_examples)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 90, in predict\n",
            "    predictions = self.post_process_function(test_examples, test_dataset, output.predictions, \"predict\")\n",
            "NameError: name 'test_examples' is not defined\n",
            "100% 1/1 [00:00<00:00,  3.64it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7I5mXl4kKl7",
        "outputId": "8078cad6-969c-4d7b-bdc0-d286187cd281"
      },
      "source": [
        "!python transformers/examples/pytorch/question-answering/run_qa_beam_search.py \\\n",
        "--model_name_or_path xlnet-large-cased \\\n",
        "--train_file transformers/tests/fixtures/tests_samples/SQUAD/sample.json \\\n",
        "--validation_file transformers/tests/fixtures/tests_samples/SQUAD/sample.json \\\n",
        "--test_file transformers/tests/fixtures/tests_samples/SQUAD/sample.json \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--max_predict_samples 5 \\\n",
        "--learning_rate 3e-5 \\\n",
        "--max_seq_length 384 \\\n",
        "--doc_stride 128 \\\n",
        "--version_2_with_negative \\\n",
        "--output_dir /tmp/debug_squad/ \\\n",
        "--overwrite_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-21 18:03:32.556921: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/21/2021 18:03:34 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/21/2021 18:03:34 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/debug_squad/, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr21_18-03-34_3feea3da0443, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/debug_squad/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=0, mp_parameters=)\n",
            "04/21/2021 18:03:34 - WARNING - datasets.builder -   Using custom data configuration default-690a5b887bd2aff6\n",
            "04/21/2021 18:03:34 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-690a5b887bd2aff6/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n",
            "[INFO|file_utils.py:1394] 2021-04-21 18:03:35,104 >> https://huggingface.co/xlnet-large-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi_aa6cdq\n",
            "Downloading: 100% 761/761 [00:00<00:00, 524kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-21 18:03:35,160 >> storing https://huggingface.co/xlnet-large-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|file_utils.py:1401] 2021-04-21 18:03:35,160 >> creating metadata file for /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:491] 2021-04-21 18:03:35,161 >> loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60\n",
            "[INFO|configuration_utils.py:527] 2021-04-21 18:03:35,161 >> Model config XLNetConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"model_type\": \"xlnet\",\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"pad_token_id\": 5,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 250\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"untie_r\": true,\n",
            "  \"use_mems_eval\": true,\n",
            "  \"use_mems_train\": false,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1394] 2021-04-21 18:03:35,215 >> https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpz7ht7cgx\n",
            "Downloading: 100% 798k/798k [00:00<00:00, 11.7MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-21 18:03:35,344 >> storing https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|file_utils.py:1401] 2021-04-21 18:03:35,344 >> creating metadata file for /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|file_utils.py:1394] 2021-04-21 18:03:35,399 >> https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphbkzikts\n",
            "Downloading: 100% 1.38M/1.38M [00:00<00:00, 14.9MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-21 18:03:35,558 >> storing https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|file_utils.py:1401] 2021-04-21 18:03:35,558 >> creating metadata file for /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-21 18:03:35,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-21 18:03:35,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-21 18:03:35,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-21 18:03:35,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-21 18:03:35,721 >> loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|file_utils.py:1394] 2021-04-21 18:03:35,871 >> https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpy78zr_es\n",
            "Downloading: 100% 1.44G/1.44G [00:30<00:00, 47.9MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-21 18:04:06,454 >> storing https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[INFO|file_utils.py:1401] 2021-04-21 18:04:06,455 >> creating metadata file for /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[INFO|modeling_utils.py:1075] 2021-04-21 18:04:06,456 >> loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776\n",
            "[WARNING|modeling_utils.py:1196] 2021-04-21 18:04:19,078 >> Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForQuestionAnswering: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1207] 2021-04-21 18:04:19,079 >> Some weights of XLNetForQuestionAnswering were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['start_logits.dense.weight', 'start_logits.dense.bias', 'end_logits.dense_0.weight', 'end_logits.dense_0.bias', 'end_logits.LayerNorm.weight', 'end_logits.LayerNorm.bias', 'end_logits.dense_1.weight', 'end_logits.dense_1.bias', 'answer_class.dense_0.weight', 'answer_class.dense_0.bias', 'answer_class.dense_1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 1/1 [00:00<00:00, 102.92ba/s]\n",
            "100% 1/1 [00:00<00:00, 67.68ba/s]\n",
            "100% 1/1 [00:00<00:00, 70.18ba/s]\n",
            "[INFO|trainer.py:1107] 2021-04-21 18:04:20,472 >> ***** Running training *****\n",
            "[INFO|trainer.py:1108] 2021-04-21 18:04:20,472 >>   Num examples = 5\n",
            "[INFO|trainer.py:1109] 2021-04-21 18:04:20,472 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1110] 2021-04-21 18:04:20,472 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1111] 2021-04-21 18:04:20,472 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1112] 2021-04-21 18:04:20,472 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1113] 2021-04-21 18:04:20,473 >>   Total optimization steps = 3\n",
            " 33% 1/3 [02:58<05:56, 178.41s/it]^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkdfShyWer51"
      },
      "source": [
        "## summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOPnWbhQk5T5",
        "outputId": "b3302a09-6260-4ec6-bbc2-b5efb0f3b02d"
      },
      "source": [
        "!python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
        "--model_name_or_path t5-small \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--max_predict_samples 5 \\\n",
        "--dataset_name cnn_dailymail \\\n",
        "--dataset_config \"3.0.0\" \\\n",
        "--source_prefix \"summarize: \" \\\n",
        "--output_dir /tmp/tst-summarization \\\n",
        "--per_device_train_batch_size=4 \\\n",
        "--per_device_eval_batch_size=4 \\\n",
        "--overwrite_output_dir \\\n",
        "--predict_with_generate"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 04:48:37.563419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/22/2021 04:48:40 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/22/2021 04:48:40 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/tmp/tst-summarization', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Apr22_04-48-40_28090cecc987', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/tst-summarization', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "Downloading: 9.35kB [00:00, 6.29MB/s]       \n",
            "Downloading: 9.50kB [00:00, 5.88MB/s]       \n",
            "Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234...\n",
            "Downloading: 159MB [00:05, 28.6MB/s]\n",
            "Downloading: 376MB [00:14, 25.5MB/s]\n",
            "Downloading: 2.11MB [00:00, 30.1MB/s]      \n",
            "Downloading: 46.4MB [00:00, 74.8MB/s]\n",
            "Downloading: 2.43MB [00:00, 41.3MB/s]      \n",
            "Dataset cnn_dailymail downloaded and prepared to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/t5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpstbyello\n",
            "Downloading: 100% 1.20k/1.20k [00:00<00:00, 959kB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "creating metadata file for /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "https://huggingface.co/t5-small/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxv3o0_71\n",
            "Downloading: 100% 792k/792k [00:00<00:00, 2.54MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "creating metadata file for /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "https://huggingface.co/t5-small/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8ukdd128\n",
            "Downloading: 100% 1.39M/1.39M [00:00<00:00, 3.71MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "creating metadata file for /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "https://huggingface.co/t5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5o7xf2bg\n",
            "Downloading: 100% 242M/242M [00:14<00:00, 17.2MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "creating metadata file for /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 37.13ba/s]\n",
            "100% 1/1 [00:00<00:00, 58.12ba/s]\n",
            "100% 1/1 [00:00<00:00, 53.04ba/s]\n",
            "Downloading: 5.61kB [00:00, 3.65MB/s]       \n",
            "***** Running training *****\n",
            "  Num examples = 5\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6\n",
            "100% 6/6 [01:36<00:00, 15.81s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 96.6873, 'train_samples_per_second': 0.062, 'epoch': 3.0}\n",
            "100% 6/6 [01:36<00:00, 16.11s/it]\n",
            "Saving model checkpoint to /tmp/tst-summarization\n",
            "Configuration saved in /tmp/tst-summarization/config.json\n",
            "Model weights saved in /tmp/tst-summarization/pytorch_model.bin\n",
            "tokenizer config file saved in /tmp/tst-summarization/tokenizer_config.json\n",
            "Special tokens file saved in /tmp/tst-summarization/special_tokens_map.json\n",
            "Copy vocab file to /tmp/tst-summarization/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                      =        3.0\n",
            "  init_mem_cpu_alloc_delta   =        0MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  train_mem_cpu_alloc_delta  =     5533MB\n",
            "  train_mem_cpu_peaked_delta =        0MB\n",
            "  train_runtime              = 0:01:36.68\n",
            "  train_samples              =          5\n",
            "  train_samples_per_second   =      0.062\n",
            "04/22/2021 04:53:41 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 4\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:965: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
            "  UserWarning,\n",
            "100% 2/2 [00:07<00:00,  3.87s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        3.0\n",
            "  eval_gen_len              =       60.8\n",
            "  eval_loss                 =     2.5208\n",
            "  eval_mem_cpu_alloc_delta  =       19MB\n",
            "  eval_mem_cpu_peaked_delta =        0MB\n",
            "  eval_rouge1               =    18.5644\n",
            "  eval_rouge2               =     5.5108\n",
            "  eval_rougeL               =    16.4633\n",
            "  eval_rougeLsum            =    18.2055\n",
            "  eval_runtime              = 0:00:36.96\n",
            "  eval_samples              =          5\n",
            "  eval_samples_per_second   =      0.135\n",
            "04/22/2021 04:54:18 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 5\n",
            "  Batch size = 4\n",
            "100% 2/2 [00:10<00:00,  5.19s/it]***** predict metrics *****\n",
            "  predict_gen_len            =       69.4\n",
            "  predict_loss               =     2.1224\n",
            "  predict_rouge1             =    26.5342\n",
            "  predict_rouge2             =     8.2051\n",
            "  predict_rougeL             =    18.1135\n",
            "  predict_rougeLsum          =    23.2219\n",
            "  predict_runtime            = 0:00:36.59\n",
            "  predict_samples            =          5\n",
            "  predict_samples_per_second =      0.137\n",
            "  test_mem_cpu_alloc_delta   =        2MB\n",
            "  test_mem_cpu_peaked_delta  =        0MB\n",
            "Traceback (most recent call last):\n",
            "  File \"transformers/examples/pytorch/summarization/run_summarization.py\", line 593, in <module>\n",
            "    main()\n",
            "  File \"transformers/examples/pytorch/summarization/run_summarization.py\", line 582, in main\n",
            "    writer.write(\"\\n\".join(test_preds))\n",
            "NameError: name 'test_preds' is not defined\n",
            "100% 2/2 [00:11<00:00,  5.54s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMuDiD48etuW"
      },
      "source": [
        "## text-classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MEiv6RAocUH",
        "outputId": "d206d67f-5056-4f8f-85f5-c705322f1b50"
      },
      "source": [
        "!python transformers/examples/pytorch/text-classification/run_glue.py \\\n",
        "--model_name_or_path distilbert-base-cased \\\n",
        "--task_name mrpc \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--max_predict_samples 5 \\\n",
        "--max_seq_length 128 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--output_dir /tmp/mrpc/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 04:54:59.251458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/22/2021 04:55:01 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/22/2021 04:55:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/mrpc/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr22_04-55-01_28090cecc987, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/mrpc/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=0, mp_parameters=)\n",
            "Downloading: 28.8kB [00:00, 16.8MB/s]       \n",
            "Downloading: 28.7kB [00:00, 19.6MB/s]       \n",
            "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
            "Downloading: 6.22kB [00:00, 4.34MB/s]\n",
            "Downloading: 1.05MB [00:00, 7.43MB/s]\n",
            "Downloading: 441kB [00:00, 4.39MB/s]\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:55:04,877 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:55:04,878 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:55:05,078 >> loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:55:05,079 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:55:06,248 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:55:06,248 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:55:06,248 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:55:06,248 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:55:06,248 >> loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
            "[INFO|modeling_utils.py:1075] 2021-04-22 04:55:06,478 >> loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
            "[WARNING|modeling_utils.py:1196] 2021-04-22 04:55:13,065 >> Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1207] 2021-04-22 04:55:13,065 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 4/4 [00:00<00:00,  7.88ba/s]\n",
            "100% 1/1 [00:00<00:00, 18.15ba/s]\n",
            "100% 2/2 [00:00<00:00,  8.63ba/s]\n",
            "04/22/2021 04:55:13 - INFO - __main__ -   Sample 0 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 0, 'input_ids': [101, 7277, 2180, 5303, 4806, 1117, 1711, 117, 2292, 1119, 1270, 107, 1103, 7737, 107, 117, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 11336, 6732, 3384, 1106, 1140, 1112, 1178, 107, 1103, 7737, 107, 117, 7277, 2180, 5303, 4806, 1117, 1711, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}.\n",
            "04/22/2021 04:55:13 - INFO - __main__ -   Sample 4 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 4, 'input_ids': [101, 1109, 4482, 3152, 109, 123, 119, 1429, 117, 1137, 1164, 1429, 3029, 117, 1106, 1601, 5286, 1120, 109, 1626, 119, 4062, 1113, 1103, 1203, 1365, 9924, 7855, 119, 102, 153, 2349, 111, 142, 13619, 119, 6117, 4874, 109, 122, 119, 5519, 1137, 129, 3029, 1106, 109, 1626, 119, 5347, 1113, 1103, 1203, 1365, 9924, 7855, 1113, 5286, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .', 'sentence2': 'PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'}.\n",
            "04/22/2021 04:55:13 - INFO - __main__ -   Sample 2 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 2, 'input_ids': [101, 1220, 1125, 1502, 1126, 16355, 1113, 1103, 4639, 1113, 1340, 1275, 117, 4733, 1103, 6527, 1111, 4688, 117, 1119, 1896, 119, 102, 1212, 1340, 1275, 117, 1103, 2062, 112, 188, 5032, 1125, 1502, 1126, 16355, 1113, 1103, 4639, 117, 4733, 1103, 16454, 1111, 4688, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .', 'sentence2': \"On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\"}.\n",
            "Downloading: 5.75kB [00:00, 3.99MB/s]       \n",
            "[INFO|trainer.py:497] 2021-04-22 04:55:17,160 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1.\n",
            "[INFO|trainer.py:1107] 2021-04-22 04:55:17,169 >> ***** Running training *****\n",
            "[INFO|trainer.py:1108] 2021-04-22 04:55:17,169 >>   Num examples = 5\n",
            "[INFO|trainer.py:1109] 2021-04-22 04:55:17,169 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1110] 2021-04-22 04:55:17,169 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1111] 2021-04-22 04:55:17,169 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1112] 2021-04-22 04:55:17,169 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1113] 2021-04-22 04:55:17,169 >>   Total optimization steps = 3\n",
            "100% 3/3 [00:11<00:00,  3.86s/it][INFO|trainer.py:1302] 2021-04-22 04:55:28,700 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 11.5308, 'train_samples_per_second': 0.26, 'epoch': 3.0}\n",
            "100% 3/3 [00:11<00:00,  3.84s/it]\n",
            "[INFO|trainer.py:1768] 2021-04-22 04:55:28,817 >> Saving model checkpoint to /tmp/mrpc/\n",
            "[INFO|configuration_utils.py:329] 2021-04-22 04:55:28,818 >> Configuration saved in /tmp/mrpc/config.json\n",
            "[INFO|modeling_utils.py:848] 2021-04-22 04:55:29,427 >> Model weights saved in /tmp/mrpc/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1918] 2021-04-22 04:55:29,427 >> tokenizer config file saved in /tmp/mrpc/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-04-22 04:55:29,428 >> Special tokens file saved in /tmp/mrpc/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:55:29,471 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:29,471 >>   epoch                      =        3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:29,471 >>   init_mem_cpu_alloc_delta   =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:29,471 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:29,471 >>   train_mem_cpu_alloc_delta  =      844MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:29,471 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:29,471 >>   train_runtime              = 0:00:11.53\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:29,471 >>   train_samples              =          5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:29,471 >>   train_samples_per_second   =       0.26\n",
            "04/22/2021 04:55:29 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:497] 2021-04-22 04:55:29,590 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1.\n",
            "[INFO|trainer.py:1988] 2021-04-22 04:55:29,595 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1990] 2021-04-22 04:55:29,595 >>   Num examples = 5\n",
            "[INFO|trainer.py:1993] 2021-04-22 04:55:29,596 >>   Batch size = 8\n",
            "100% 1/1 [00:00<00:00, 39.84it/s]\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:55:30,721 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   epoch                     =        3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   eval_accuracy             =        0.6\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   eval_combined_score       =        0.3\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   eval_f1                   =        0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   eval_loss                 =     0.6851\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   eval_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   eval_runtime              = 0:00:01.01\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   eval_samples              =          5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:55:30,722 >>   eval_samples_per_second   =      4.939\n",
            "04/22/2021 04:55:30 - INFO - __main__ -   *** Predict ***\n",
            "transformers/examples/pytorch/text-classification/run_glue.py:506: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\n",
            "  predict_dataset.remove_columns_(\"label\")\n",
            "[INFO|trainer.py:497] 2021-04-22 04:55:30,838 >> The following columns in the prediction set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1.\n",
            "[INFO|trainer.py:1988] 2021-04-22 04:55:30,842 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:1990] 2021-04-22 04:55:30,842 >>   Num examples = 5\n",
            "[INFO|trainer.py:1993] 2021-04-22 04:55:30,842 >>   Batch size = 8\n",
            "  0% 0/1 [00:00<?, ?it/s]04/22/2021 04:55:31 - INFO - __main__ -   ***** Predict results mrpc *****\n",
            "100% 1/1 [00:00<00:00,  8.77it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot6lcfpDevM_"
      },
      "source": [
        "## text-translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF6SSJnCrHLT",
        "outputId": "ec3f84f6-f348-48b7-d4dc-50ce04465946"
      },
      "source": [
        "!python transformers/examples/pytorch/translation/run_translation.py \\\n",
        "--model_name_or_path Helsinki-NLP/opus-mt-en-ro \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--max_predict_samples 5 \\\n",
        "--source_lang en \\\n",
        "--target_lang ro \\\n",
        "--dataset_name wmt16 \\\n",
        "--dataset_config_name ro-en \\\n",
        "--output_dir /tmp/tst-translation \\\n",
        "--per_device_train_batch_size=4 \\\n",
        "--per_device_eval_batch_size=4 \\\n",
        "--overwrite_output_dir \\\n",
        "--predict_with_generate"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 04:55:34.542841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/22/2021 04:55:36 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/22/2021 04:55:36 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/tmp/tst-translation', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Apr22_04-55-36_28090cecc987', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/tmp/tst-translation', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, mp_parameters='', sortish_sampler=False, predict_with_generate=True)\n",
            "Downloading: 2.81kB [00:00, 2.21MB/s]       \n",
            "Downloading: 3.19kB [00:00, 2.55MB/s]       \n",
            "Downloading: 41.0kB [00:00, 21.9MB/s]       \n",
            "Downloading and preparing dataset wmt16/ro-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a...\n",
            "Downloading: 100% 225M/225M [00:03<00:00, 57.2MB/s]\n",
            "Downloading: 100% 23.5M/23.5M [00:02<00:00, 8.60MB/s]\n",
            "Downloading: 100% 38.7M/38.7M [00:00<00:00, 58.4MB/s]\n",
            "Dataset wmt16 downloaded and prepared to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf50oedco\n",
            "Downloading: 100% 1.13k/1.13k [00:00<00:00, 801kB/s]\n",
            "storing https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/c9aa21082ce9a9811f9545a0fc0b441e82444d82f3b2571462c42fb470eec36e.9b192a33701c4f94ad3145ff0cdda62ca61214951101372f2ddaa47cf4f4aa25\n",
            "creating metadata file for /root/.cache/huggingface/transformers/c9aa21082ce9a9811f9545a0fc0b441e82444d82f3b2571462c42fb470eec36e.9b192a33701c4f94ad3145ff0cdda62ca61214951101372f2ddaa47cf4f4aa25\n",
            "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c9aa21082ce9a9811f9545a0fc0b441e82444d82f3b2571462c42fb470eec36e.9b192a33701c4f94ad3145ff0cdda62ca61214951101372f2ddaa47cf4f4aa25\n",
            "Model config MarianConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"swish\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"MarianMTModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bad_words_ids\": [\n",
            "    [\n",
            "      59542\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_attention_heads\": 8,\n",
            "  \"decoder_ffn_dim\": 2048,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 59542,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 8,\n",
            "  \"encoder_ffn_dim\": 2048,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 512,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"marian\",\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 59542,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 59543\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/c9aa21082ce9a9811f9545a0fc0b441e82444d82f3b2571462c42fb470eec36e.9b192a33701c4f94ad3145ff0cdda62ca61214951101372f2ddaa47cf4f4aa25\n",
            "Model config MarianConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"swish\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"MarianMTModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bad_words_ids\": [\n",
            "    [\n",
            "      59542\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_attention_heads\": 8,\n",
            "  \"decoder_ffn_dim\": 2048,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 59542,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 8,\n",
            "  \"encoder_ffn_dim\": 2048,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 0,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_length\": 512,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"marian\",\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 59542,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 59543\n",
            "}\n",
            "\n",
            "https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/source.spm not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1ze9jsec\n",
            "Downloading: 100% 789k/789k [00:00<00:00, 2.51MB/s]\n",
            "storing https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/source.spm in cache at /root/.cache/huggingface/transformers/1954839338efdfd7d7cd66bb676626f3fa5404a96e4c3fd9e0d777f08fb2f7fb.2a52a89183278824d0afa78f92263dc46ca0c520a60df7071b6ae4e00fc5bef7\n",
            "creating metadata file for /root/.cache/huggingface/transformers/1954839338efdfd7d7cd66bb676626f3fa5404a96e4c3fd9e0d777f08fb2f7fb.2a52a89183278824d0afa78f92263dc46ca0c520a60df7071b6ae4e00fc5bef7\n",
            "https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/target.spm not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5iyxmij_\n",
            "Downloading: 100% 817k/817k [00:00<00:00, 2.60MB/s]\n",
            "storing https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/target.spm in cache at /root/.cache/huggingface/transformers/1e4b193c82cb1b7fe168ad6685f3a053a2b1ab3482d2d12c306282e03ff379f0.afd22f83f2170348e28be1f817974819f568155c58acd1bb3800cd7937db36a1\n",
            "creating metadata file for /root/.cache/huggingface/transformers/1e4b193c82cb1b7fe168ad6685f3a053a2b1ab3482d2d12c306282e03ff379f0.afd22f83f2170348e28be1f817974819f568155c58acd1bb3800cd7937db36a1\n",
            "https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1jk98g2t\n",
            "Downloading: 100% 1.39M/1.39M [00:00<00:00, 3.68MB/s]\n",
            "storing https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/bffa5c5398d82c200e0eeee10da8e37228615dc7fbabd429567d6ae242f5a615.5e73795abff6c04c6739872ed86c111983aa47f92d9648ac33f2d9d3643e1357\n",
            "creating metadata file for /root/.cache/huggingface/transformers/bffa5c5398d82c200e0eeee10da8e37228615dc7fbabd429567d6ae242f5a615.5e73795abff6c04c6739872ed86c111983aa47f92d9648ac33f2d9d3643e1357\n",
            "https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps6wzu7du\n",
            "Downloading: 100% 42.0/42.0 [00:00<00:00, 29.9kB/s]\n",
            "storing https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/e05a8eb93de5526dd8224c840b38615d3f79b0bf9636f74ade2807ec16ae347f.09f8e480ed5ba39c95bf16472d22478f2d1870f222433394dd0fee0ac9f1b907\n",
            "creating metadata file for /root/.cache/huggingface/transformers/e05a8eb93de5526dd8224c840b38615d3f79b0bf9636f74ade2807ec16ae347f.09f8e480ed5ba39c95bf16472d22478f2d1870f222433394dd0fee0ac9f1b907\n",
            "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/source.spm from cache at /root/.cache/huggingface/transformers/1954839338efdfd7d7cd66bb676626f3fa5404a96e4c3fd9e0d777f08fb2f7fb.2a52a89183278824d0afa78f92263dc46ca0c520a60df7071b6ae4e00fc5bef7\n",
            "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/target.spm from cache at /root/.cache/huggingface/transformers/1e4b193c82cb1b7fe168ad6685f3a053a2b1ab3482d2d12c306282e03ff379f0.afd22f83f2170348e28be1f817974819f568155c58acd1bb3800cd7937db36a1\n",
            "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/bffa5c5398d82c200e0eeee10da8e37228615dc7fbabd429567d6ae242f5a615.5e73795abff6c04c6739872ed86c111983aa47f92d9648ac33f2d9d3643e1357\n",
            "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e05a8eb93de5526dd8224c840b38615d3f79b0bf9636f74ade2807ec16ae347f.09f8e480ed5ba39c95bf16472d22478f2d1870f222433394dd0fee0ac9f1b907\n",
            "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer.json from cache at None\n",
            "https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplqakpqpn\n",
            "Downloading: 100% 301M/301M [00:05<00:00, 53.2MB/s]\n",
            "storing https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/ddea6571c8d435d3a95a0d3b003ce4ed9cd74380314ce1510adc1a2b80ec6541.ea59afa9489e25f746dc0446b0ef6874b3b9e704d9d077620684ec467f86be9d\n",
            "creating metadata file for /root/.cache/huggingface/transformers/ddea6571c8d435d3a95a0d3b003ce4ed9cd74380314ce1510adc1a2b80ec6541.ea59afa9489e25f746dc0446b0ef6874b3b9e704d9d077620684ec467f86be9d\n",
            "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ddea6571c8d435d3a95a0d3b003ce4ed9cd74380314ce1510adc1a2b80ec6541.ea59afa9489e25f746dc0446b0ef6874b3b9e704d9d077620684ec467f86be9d\n",
            "All model checkpoint weights were used when initializing MarianMTModel.\n",
            "\n",
            "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-ro.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 255.91ba/s]\n",
            "100% 1/1 [00:00<00:00, 297.13ba/s]\n",
            "100% 1/1 [00:00<00:00, 282.25ba/s]\n",
            "Downloading: 5.40kB [00:00, 3.59MB/s]       \n",
            "***** Running training *****\n",
            "  Num examples = 5\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6\n",
            "100% 6/6 [00:09<00:00,  1.50s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 9.1755, 'train_samples_per_second': 0.654, 'epoch': 3.0}\n",
            "100% 6/6 [00:09<00:00,  1.53s/it]\n",
            "Saving model checkpoint to /tmp/tst-translation\n",
            "Configuration saved in /tmp/tst-translation/config.json\n",
            "Model weights saved in /tmp/tst-translation/pytorch_model.bin\n",
            "tokenizer config file saved in /tmp/tst-translation/tokenizer_config.json\n",
            "Special tokens file saved in /tmp/tst-translation/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                      =        3.0\n",
            "  init_mem_cpu_alloc_delta   =        0MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  train_mem_cpu_alloc_delta  =     1015MB\n",
            "  train_mem_cpu_peaked_delta =        0MB\n",
            "  train_runtime              = 0:00:09.17\n",
            "  train_samples              =          5\n",
            "  train_samples_per_second   =      0.654\n",
            "04/22/2021 04:56:46 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5\n",
            "  Batch size = 4\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:965: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
            "  UserWarning,\n",
            "100% 2/2 [00:02<00:00,  1.35s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        3.0\n",
            "  eval_bleu                 =    27.7241\n",
            "  eval_gen_len              =       39.2\n",
            "  eval_loss                 =     0.9804\n",
            "  eval_mem_cpu_alloc_delta  =        1MB\n",
            "  eval_mem_cpu_peaked_delta =        0MB\n",
            "  eval_runtime              = 0:00:11.54\n",
            "  eval_samples              =          5\n",
            "  eval_samples_per_second   =      0.433\n",
            "04/22/2021 04:56:58 - INFO - __main__ -   *** Predict ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 5\n",
            "  Batch size = 4\n",
            "100% 2/2 [00:02<00:00,  1.41s/it]***** predict metrics *****\n",
            "  predict_samples           =          5\n",
            "  test_bleu                 =    31.1522\n",
            "  test_gen_len              =       40.0\n",
            "  test_loss                 =      1.423\n",
            "  test_mem_cpu_alloc_delta  =        0MB\n",
            "  test_mem_cpu_peaked_delta =        0MB\n",
            "  test_runtime              = 0:00:11.85\n",
            "  test_samples_per_second   =      0.422\n",
            "100% 2/2 [00:02<00:00,  1.49s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0jyisZ2ewm1"
      },
      "source": [
        "## token-classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BEeAVLaensq",
        "outputId": "3a37788b-3b9b-429f-d67c-ad6ea9cc9c60"
      },
      "source": [
        "!python transformers/examples/pytorch/token-classification/run_ner.py \\\n",
        "--model_name_or_path bert-base-uncased \\\n",
        "--dataset_name conll2003 \\\n",
        "--output_dir /tmp/test-ner \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict \\\n",
        "--max_train_samples 5 \\\n",
        "--max_eval_samples 5 \\\n",
        "--max_predict_samples 5 \\"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-22 04:57:12.900481: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "04/22/2021 04:57:14 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "04/22/2021 04:57:14 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/test-ner, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr22_04-57-14_28090cecc987, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/test-ner, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, _n_gpu=0, mp_parameters=)\n",
            "Downloading: 9.52kB [00:00, 6.43MB/s]       \n",
            "Downloading: 4.18kB [00:00, 3.07MB/s]       \n",
            "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6...\n",
            "Downloading: 3.28MB [00:00, 53.6MB/s]      \n",
            "Downloading: 827kB [00:00, 49.7MB/s]       \n",
            "Downloading: 748kB [00:00, 35.1MB/s]       \n",
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:57:23,392 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpiso0vl81\n",
            "Downloading: 100% 433/433 [00:00<00:00, 303kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:57:23,593 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:57:23,593 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:57:23,594 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:57:23,594 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:491] 2021-04-22 04:57:23,801 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|configuration_utils.py:527] 2021-04-22 04:57:23,802 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.6.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:57:24,003 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8v3q7fs1\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 953kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:57:24,447 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:57:24,447 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:57:24,647 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvb7c4rlc\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.54MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:57:25,154 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:57:25,154 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:57:25,752 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmhbimrlo\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 19.5kB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:57:25,954 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:57:25,954 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:57:25,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:57:25,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:57:25,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:57:25,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1713] 2021-04-22 04:57:25,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|file_utils.py:1394] 2021-04-22 04:57:26,182 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpki0uitkx\n",
            "Downloading: 100% 440M/440M [00:24<00:00, 17.9MB/s]\n",
            "[INFO|file_utils.py:1398] 2021-04-22 04:57:51,033 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|file_utils.py:1401] 2021-04-22 04:57:51,033 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|modeling_utils.py:1075] 2021-04-22 04:57:51,034 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1196] 2021-04-22 04:57:54,899 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1207] 2021-04-22 04:57:54,899 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 1/1 [00:00<00:00, 139.37ba/s]\n",
            "100% 1/1 [00:00<00:00, 253.31ba/s]\n",
            "100% 1/1 [00:00<00:00, 296.52ba/s]\n",
            "Downloading: 6.34kB [00:00, 4.09MB/s]       \n",
            "[INFO|trainer.py:497] 2021-04-22 04:57:56,191 >> The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags, id, pos_tags, chunk_tags.\n",
            "[INFO|trainer.py:1107] 2021-04-22 04:57:56,199 >> ***** Running training *****\n",
            "[INFO|trainer.py:1108] 2021-04-22 04:57:56,200 >>   Num examples = 5\n",
            "[INFO|trainer.py:1109] 2021-04-22 04:57:56,200 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1110] 2021-04-22 04:57:56,200 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1111] 2021-04-22 04:57:56,200 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1112] 2021-04-22 04:57:56,200 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1113] 2021-04-22 04:57:56,200 >>   Total optimization steps = 3\n",
            "100% 3/3 [00:10<00:00,  3.56s/it][INFO|trainer.py:1302] 2021-04-22 04:58:06,548 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 10.3481, 'train_samples_per_second': 0.29, 'epoch': 3.0}\n",
            "100% 3/3 [00:10<00:00,  3.44s/it]\n",
            "[INFO|trainer.py:1768] 2021-04-22 04:58:06,669 >> Saving model checkpoint to /tmp/test-ner\n",
            "[INFO|configuration_utils.py:329] 2021-04-22 04:58:06,670 >> Configuration saved in /tmp/test-ner/config.json\n",
            "[INFO|modeling_utils.py:848] 2021-04-22 04:58:07,739 >> Model weights saved in /tmp/test-ner/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1918] 2021-04-22 04:58:07,740 >> tokenizer config file saved in /tmp/test-ner/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-04-22 04:58:07,740 >> Special tokens file saved in /tmp/test-ner/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:58:07,785 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:07,785 >>   epoch                      =        3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:07,786 >>   init_mem_cpu_alloc_delta   =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:07,786 >>   init_mem_cpu_peaked_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:07,786 >>   train_mem_cpu_alloc_delta  =     1005MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:07,786 >>   train_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:07,786 >>   train_runtime              = 0:00:10.34\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:07,786 >>   train_samples              =          5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:07,786 >>   train_samples_per_second   =       0.29\n",
            "04/22/2021 04:58:07 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:497] 2021-04-22 04:58:07,923 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags, id, pos_tags, chunk_tags.\n",
            "[INFO|trainer.py:1988] 2021-04-22 04:58:07,928 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1990] 2021-04-22 04:58:07,928 >>   Num examples = 5\n",
            "[INFO|trainer.py:1993] 2021-04-22 04:58:07,928 >>   Batch size = 8\n",
            "  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "100% 1/1 [00:00<00:00, 71.06it/s]\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:58:08,844 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   epoch                     =        3.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   eval_accuracy             =     0.7561\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   eval_f1                   =        0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   eval_loss                 =      1.669\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   eval_mem_cpu_alloc_delta  =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   eval_mem_cpu_peaked_delta =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   eval_precision            =        0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   eval_recall               =        0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   eval_runtime              = 0:00:00.79\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,844 >>   eval_samples              =          5\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:08,845 >>   eval_samples_per_second   =      6.312\n",
            "04/22/2021 04:58:08 - INFO - __main__ -   *** Predict ***\n",
            "[INFO|trainer.py:497] 2021-04-22 04:58:08,963 >> The following columns in the prediction set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, ner_tags, id, pos_tags, chunk_tags.\n",
            "[INFO|trainer.py:1988] 2021-04-22 04:58:08,968 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:1990] 2021-04-22 04:58:08,968 >>   Num examples = 5\n",
            "[INFO|trainer.py:1993] 2021-04-22 04:58:08,968 >>   Batch size = 8\n",
            "  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "[INFO|trainer_pt_utils.py:898] 2021-04-22 04:58:09,611 >> ***** predict metrics *****\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:09,611 >>   predict_accuracy           =     0.7857\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:09,612 >>   predict_f1                 =        0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:09,612 >>   predict_loss               =     1.4666\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:09,612 >>   predict_precision          =        0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:09,612 >>   predict_recall             =        0.0\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:09,612 >>   predict_runtime            = 0:00:00.52\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:09,612 >>   predict_samples_per_second =      9.584\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:09,612 >>   test_mem_cpu_alloc_delta   =        0MB\n",
            "[INFO|trainer_pt_utils.py:903] 2021-04-22 04:58:09,612 >>   test_mem_cpu_peaked_delta  =        0MB\n",
            "100% 1/1 [00:00<00:00,  7.46it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKpCx5dusRNO"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}