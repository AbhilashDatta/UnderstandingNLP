{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Check_run_tf_squad.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNm9QBqX1jjKv38xwdV7c6Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/UnderstandingNLP/blob/master/Check_run_tf_squad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChJDRrOw9FPV",
        "outputId": "1fd49576-ed04-4413-8f06-ef0a2431085a"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\r\n",
        "%cd transformers\r\n",
        "!pip install .\r\n",
        "!pip install -r ./examples/question-answering/requirements.txt\r\n",
        "%cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 65438, done.\u001b[K\n",
            "remote: Total 65438 (delta 0), reused 0 (delta 0), pack-reused 65438\u001b[K\n",
            "Receiving objects: 100% (65438/65438), 48.92 MiB | 22.71 MiB/s, done.\n",
            "Resolving deltas: 100% (46409/46409), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 10.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 31.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.4.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.4.0.dev0-cp37-none-any.whl size=1896183 sha256=99986c6bf580ae0eb91cbeb29f014d3d9f68083ff9083dcaa82377e77af0d40b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5d4d8f92/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=4755be18525c2e64ac50945ab0481789707ffed7867a1c68aa05ec91fb8476ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.0.dev0\n",
            "Collecting datasets>=1.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/73/742d17d8a9a1c639132affccc9250f0743e484cbf263ede6ddcbe34ef212/datasets-1.4.1-py3-none-any.whl (186kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.7.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (0.70.11.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (4.41.1)\n",
            "Collecting huggingface-hub==0.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/b5/93/7cb0755c62c36cdadc70c79a95681df685b52cbaf76c724facb6ecac3272/huggingface_hub-0.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (1.1.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (1.19.5)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.2->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (1.15.0)\n",
            "Installing collected packages: huggingface-hub, xxhash, fsspec, datasets\n",
            "Successfully installed datasets-1.4.1 fsspec-0.8.7 huggingface-hub-0.0.2 xxhash-2.0.0\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX3N0hWf-Pyk"
      },
      "source": [
        "## TF_SQUAD:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiQDnhSv89Nb",
        "outputId": "a67e8517-5f67-4a4e-cb39-04eaead9ec49"
      },
      "source": [
        "!python ./transformers/examples/question-answering/run_tf_squad.py --model_name_or_path bert-base-uncased --output_dir model --max_seq_length 384 --num_train_epochs 2 --per_gpu_train_batch_size 8 --per_gpu_eval_batch_size 16 --do_train --logging_dir logs --logging_steps 10 --learning_rate 3e-5 --doc_stride 128"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-05 10:40:07.463982: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "[INFO|training_args.py:618] 2021-03-05 10:40:14,053 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:542] 2021-03-05 10:40:14,137 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "[INFO|training_args_tf.py:192] 2021-03-05 10:40:14,138 >> Tensorflow: setting up strategy\n",
            "2021-03-05 10:40:14.144850: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-05 10:40:14.145252: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-05 10:40:14.145672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-05 10:40:14.146539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2021-03-05 10:40:14.146583: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-05 10:40:14.260809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-03-05 10:40:14.260939: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-03-05 10:40:14.431266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-03-05 10:40:14.466593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-03-05 10:40:14.731232: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-03-05 10:40:14.748795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-03-05 10:40:14.753585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-03-05 10:40:14.753740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-05 10:40:14.754556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-05 10:40:14.758926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-03-05 10:40:14.759954: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-05 10:40:14.760088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-05 10:40:14.760876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n",
            "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
            "2021-03-05 10:40:14.760917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-05 10:40:14.760974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-03-05 10:40:14.761019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-03-05 10:40:14.761061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-03-05 10:40:14.761103: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-03-05 10:40:14.761162: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-03-05 10:40:14.761253: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-03-05 10:40:14.761294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-03-05 10:40:14.761403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-05 10:40:14.762145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-05 10:40:14.762816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-03-05 10:40:14.765785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-05 10:40:19.045089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-03-05 10:40:19.045142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-03-05 10:40:19.045161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-03-05 10:40:19.053066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-05 10:40:19.053853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-05 10:40:19.054524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-05 10:40:19.055419: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-03-05 10:40:19.055490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10637 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "03/05/2021 10:40:19 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n",
            "03/05/2021 10:40:19 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='model', overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='logs', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, tpu_name=None, tpu_zone=None, gcp_project=None, poly_power=1.0, xla=False)\n",
            "03/05/2021 10:40:19 - INFO - filelock -   Lock 139898421074640 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n",
            "[INFO|file_utils.py:1327] 2021-03-05 10:40:19,346 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyf1jx0by\n",
            "Downloading: 100% 433/433 [00:00<00:00, 380kB/s]\n",
            "[INFO|file_utils.py:1331] 2021-03-05 10:40:19,606 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|file_utils.py:1334] 2021-03-05 10:40:19,606 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "03/05/2021 10:40:19 - INFO - filelock -   Lock 139898421074640 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n",
            "[INFO|configuration_utils.py:459] 2021-03-05 10:40:19,607 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|configuration_utils.py:495] 2021-03-05 10:40:19,608 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:459] 2021-03-05 10:40:19,871 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n",
            "[INFO|configuration_utils.py:495] 2021-03-05 10:40:19,872 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "03/05/2021 10:40:20 - INFO - filelock -   Lock 139898406328592 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "[INFO|file_utils.py:1327] 2021-03-05 10:40:20,142 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpuyshc160\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 711kB/s]\n",
            "[INFO|file_utils.py:1331] 2021-03-05 10:40:20,728 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1334] 2021-03-05 10:40:20,728 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "03/05/2021 10:40:20 - INFO - filelock -   Lock 139898406328592 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "[INFO|tokenization_utils_base.py:1716] 2021-03-05 10:40:20,729 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "03/05/2021 10:40:21 - INFO - filelock -   Lock 139898406278608 acquired on /root/.cache/huggingface/transformers/775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5.lock\n",
            "[INFO|file_utils.py:1327] 2021-03-05 10:40:21,024 >> https://huggingface.co/bert-base-uncased/resolve/main/tf_model.h5 not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpx4_h0it0\n",
            "Downloading: 100% 536M/536M [00:12<00:00, 42.6MB/s]\n",
            "[INFO|file_utils.py:1331] 2021-03-05 10:40:33,679 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tf_model.h5 in cache at /root/.cache/huggingface/transformers/775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5\n",
            "[INFO|file_utils.py:1334] 2021-03-05 10:40:33,680 >> creating metadata file for /root/.cache/huggingface/transformers/775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5\n",
            "03/05/2021 10:40:33 - INFO - filelock -   Lock 139898406278608 released on /root/.cache/huggingface/transformers/775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5.lock\n",
            "[INFO|modeling_tf_utils.py:1222] 2021-03-05 10:40:33,680 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5\n",
            "2021-03-05 10:40:34.018525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-03-05 10:40:36.086929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "[WARNING|modeling_tf_utils.py:1270] 2021-03-05 10:40:37,095 >> All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
            "\n",
            "[WARNING|modeling_tf_utils.py:1274] 2021-03-05 10:40:37,096 >> Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "03/05/2021 10:40:37 - INFO - absl -   No config specified, defaulting to first: squad/v1.1\n",
            "2021-03-05 10:40:37.901847: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2021-03-05 10:40:38.033752: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2021-03-05 10:40:38.154934: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "03/05/2021 10:40:38 - INFO - absl -   Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: squad/v1.1/2.0.0\n",
            "2021-03-05 10:40:38.284892: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "03/05/2021 10:40:38 - INFO - absl -   Load dataset info from /tmp/tmpjbayxryjtfds\n",
            "03/05/2021 10:40:38 - INFO - absl -   Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
            "03/05/2021 10:40:38 - INFO - absl -   Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
            "03/05/2021 10:40:38 - INFO - absl -   Generating dataset squad (/root/tensorflow_datasets/squad/v1.1/2.0.0)\n",
            "\u001b[1mDownloading and preparing dataset squad/v1.1/2.0.0 (download: 33.51 MiB, generated: 94.04 MiB, total: 127.55 MiB) to /root/tensorflow_datasets/squad/v1.1/2.0.0...\u001b[0m\n",
            "2021-03-05 10:40:38.809708: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2021-03-05 10:40:38.968249: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A03/05/2021 10:40:39 - INFO - absl -   Downloading https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json into /root/tensorflow_datasets/downloads/rajpurkar_SQuAD-explorer_dev-v1.1nVRimH71-BT-FaNpwXJPbsOaIBizticanX0lmGhsov8.json.tmp.37fad1347d884ef48507f457370272ab...\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A03/05/2021 10:40:39 - INFO - absl -   Downloading https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json into /root/tensorflow_datasets/downloads/rajpurkar_SQuAD-explorer_train-v1.1uLsZc14btZFRCgHMAy9Mn5abwO6wga4bMozTBvOyQAg.json.tmp.5ea806ab71ed4d13b75bdccb15c31be6...\n",
            "Dl Completed...:   0% 0/2 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/2 [00:00<?, ? url/s]\n",
            "Dl Size...:   0% 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0% 0/2 [00:00<?, ? url/s]\n",
            "Dl Size...: 100% 1/1 [00:00<00:00,  3.85 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/2 [00:00<?, ? url/s]\n",
            "Dl Size...: 2 MiB [00:00,  3.85 MiB/s]         \u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/2 [00:00<?, ? url/s]\n",
            "Dl Size...: 3 MiB [00:00,  3.85 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/2 [00:00<?, ? url/s]\n",
            "Dl Size...: 4 MiB [00:00,  3.85 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...: 4 MiB [00:00,  3.85 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A03/05/2021 10:40:39 - INFO - absl -   Skipping extraction for /root/tensorflow_datasets/downloads/rajpurkar_SQuAD-explorer_dev-v1.1nVRimH71-BT-FaNpwXJPbsOaIBizticanX0lmGhsov8.json (method=NO_EXTRACT).\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...:  50% 4/8 [00:00<00:01,  3.85 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...:  62% 5/8 [00:00<00:00,  4.37 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...:  75% 6/8 [00:00<00:00,  4.37 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...:  88% 7/8 [00:00<00:00,  4.37 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...: 100% 8/8 [00:00<00:00,  4.37 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...: 9 MiB [00:00,  4.37 MiB/s]         \u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...: 10 MiB [00:00,  4.37 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...: 11 MiB [00:00,  4.37 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  50% 1/2 [00:00<00:00,  3.18 url/s]\n",
            "Dl Size...: 12 MiB [00:00,  6.07 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 13 MiB [00:01,  6.07 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 14 MiB [00:01,  6.07 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 15 MiB [00:01,  6.07 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 16 MiB [00:01,  6.07 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 17 MiB [00:01,  6.07 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 18 MiB [00:01,  6.07 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 19 MiB [00:01,  6.07 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 20 MiB [00:01,  8.39 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 21 MiB [00:01,  8.39 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 22 MiB [00:01,  8.39 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 23 MiB [00:01,  8.39 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 24 MiB [00:01,  8.39 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 25 MiB [00:01,  8.39 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 26 MiB [00:01,  8.39 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 27 MiB [00:01, 11.40 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 28 MiB [00:01, 11.40 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 29 MiB [00:01, 11.40 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 30 MiB [00:01, 11.40 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 31 MiB [00:01, 11.40 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  50% 1/2 [00:01<00:00,  3.18 url/s]\n",
            "Dl Size...: 32 MiB [00:01, 11.40 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 2/2 [00:01<00:00,  1.97 url/s]\n",
            "Dl Size...: 32 MiB [00:01, 11.40 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A03/05/2021 10:40:40 - INFO - absl -   Skipping extraction for /root/tensorflow_datasets/downloads/rajpurkar_SQuAD-explorer_train-v1.1uLsZc14btZFRCgHMAy9Mn5abwO6wga4bMozTBvOyQAg.json (method=NO_EXTRACT).\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\n",
            "\n",
            "Dl Size...: 32 MiB [00:01, 25.06 MiB/s]\n",
            "\n",
            "Dl Completed...: 100% 2/2 [00:01<00:00,  1.57 url/s]\n",
            "03/05/2021 10:40:40 - INFO - absl -   Generating split train\n",
            "0 examples [00:00, ? examples/s]03/05/2021 10:40:40 - INFO - absl -   generating examples from = /root/tensorflow_datasets/downloads/rajpurkar_SQuAD-explorer_train-v1.1uLsZc14btZFRCgHMAy9Mn5abwO6wga4bMozTBvOyQAg.json\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/squad/v1.1/2.0.0.incomplete3GDDA2/squad-train.tfrecord\n",
            " 68% 59993/87599 [00:00<00:00, 104883.90 examples/s]03/05/2021 10:41:34 - INFO - absl -   Done writing /root/tensorflow_datasets/squad/v1.1/2.0.0.incomplete3GDDA2/squad-train.tfrecord. Shard lengths: [87599]\n",
            "03/05/2021 10:41:34 - INFO - absl -   Generating split validation\n",
            "0 examples [00:00, ? examples/s]03/05/2021 10:41:34 - INFO - absl -   generating examples from = /root/tensorflow_datasets/downloads/rajpurkar_SQuAD-explorer_dev-v1.1nVRimH71-BT-FaNpwXJPbsOaIBizticanX0lmGhsov8.json\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/squad/v1.1/2.0.0.incomplete3GDDA2/squad-validation.tfrecord\n",
            "  0% 0/10570 [00:00<?, ? examples/s]03/05/2021 10:41:42 - INFO - absl -   Done writing /root/tensorflow_datasets/squad/v1.1/2.0.0.incomplete3GDDA2/squad-validation.tfrecord. Shard lengths: [10570]\n",
            "03/05/2021 10:41:42 - INFO - absl -   Skipping computing stats for mode ComputeStatsMode.SKIP.\n",
            "\u001b[1mDataset squad downloaded and prepared to /root/tensorflow_datasets/squad/v1.1/2.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "03/05/2021 10:41:42 - INFO - absl -   Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/squad/v1.1/2.0.0\n",
            "  0% 0/87599 [00:00<?, ?it/s]2021-03-05 10:41:42.954601: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-03-05 10:41:42.956943: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz\n",
            "100% 87599/87599 [01:48<00:00, 809.12it/s]\n",
            "convert squad examples to features:   6% 5153/87599 [01:09<19:00, 72.26it/s][WARNING|squad.py:118] 2021-03-05 10:44:40,705 >> Could not find answer: 'municipal building and' vs. 'a municipal building'\n",
            "convert squad examples to features:  59% 51425/87599 [11:17<08:05, 74.46it/s][WARNING|squad.py:118] 2021-03-05 10:54:49,441 >> Could not find answer: 'message stick,' vs. 'a message stick'\n",
            "convert squad examples to features: 100% 87599/87599 [19:21<00:00, 75.43it/s]\n",
            "add example index and unique id: 100% 87599/87599 [00:00<00:00, 784807.05it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"./transformers/examples/question-answering/run_tf_squad.py\", line 257, in <module>\n",
            "    main()\n",
            "  File \"./transformers/examples/question-answering/run_tf_squad.py\", line 239, in main\n",
            "    eval_dataset = eval_dataset.apply(tf.data.experimental.assert_cardinality(len(eval_examples)))\n",
            "AttributeError: 'NoneType' object has no attribute 'apply'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4KR66Qd9YEs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}