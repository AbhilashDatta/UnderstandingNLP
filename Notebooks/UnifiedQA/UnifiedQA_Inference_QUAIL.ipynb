{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnifiedQA_Inference_QUAIL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlg33aHP28pMHihlK6woLx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9eea2f96205d498a8a4cf0ec7d006a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a48c8f21e0544e5f8a573644ae89972a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c259cd435b24cff8187ec0f93c90c8b",
              "IPY_MODEL_ba7932c13c054be0a57da2aeac45e909"
            ]
          }
        },
        "a48c8f21e0544e5f8a573644ae89972a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c259cd435b24cff8187ec0f93c90c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b811845c269e45079051ec27aeb0644c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1276,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1276,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7582f4a7c2d4585ba40c4c586cff9ed"
          }
        },
        "ba7932c13c054be0a57da2aeac45e909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f9122f0c302b4574864191ca530c2bbf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1276/1276 [07:20&lt;00:00,  2.90it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a52c3d0b594f4946a8f416d9773b42ff"
          }
        },
        "b811845c269e45079051ec27aeb0644c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7582f4a7c2d4585ba40c4c586cff9ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9122f0c302b4574864191ca530c2bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a52c3d0b594f4946a8f416d9773b42ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/UnderstandingNLP/blob/master/UnifiedQA_Inference_QUAIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOfucoEskrH-"
      },
      "source": [
        "# UnifiedQA Performance on QUAIL Dataset:\r\n",
        "We want to check the performance of UnifiedQA base model on QUAIL. QUAIL Dataset is having total 10 different type of questions. \r\n",
        "\r\n",
        "Here are the Steps we will perform in the notebook,\r\n",
        "1. We will analyse QUAIL Dataset\r\n",
        "2. Do Preprocessing According to UnifiedQA Input Requirement\r\n",
        "3. Perform Inference on UnifiedQA\r\n",
        "4. Evaluate the Performance \r\n",
        "\r\n",
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVHaaH0dkq0N"
      },
      "source": [
        "!pip install -q transformers==3.1.0\r\n",
        "!pip install -q datasets"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKEgSZhkk6uX"
      },
      "source": [
        "from datasets import load_dataset\r\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, T5ForConditionalGeneration\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "tqdm.pandas()"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A6JEIEvgk-Zt",
        "outputId": "d603ba6a-ac5d-4a21-f212-37076ff4d635"
      },
      "source": [
        "import transformers\r\n",
        "transformers.__version__"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Tbdf82pEGrgt",
        "outputId": "3a775b77-8c8b-424c-928c-1874fa050545"
      },
      "source": [
        "from torch import cuda\r\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\r\n",
        "device"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGH47WpL70PP"
      },
      "source": [
        "## Task1. Dataset Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQaC6P5zk_uj"
      },
      "source": [
        "### 1. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rkXuwRmkHeX",
        "outputId": "6c8a6d97-f3f8-423a-f493-dabb5f27fa38"
      },
      "source": [
        "dataset = load_dataset(\"quail\")"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset quail (/root/.cache/huggingface/datasets/quail/quail/1.3.0/17ee3da60d3c4939a1679994e241f120c7c007baee8cbfb226baf6e106095101)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wZ4BzSOkec1"
      },
      "source": [
        "train_dataset = dataset['train']\r\n",
        "validation_dataset = dataset['validation']\r\n",
        "challenge_dataset = dataset['challenge']"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ82b1HGlErV",
        "outputId": "56b2b99e-23e2-4632-b482-a14942b452d6"
      },
      "source": [
        "print(\"Train\", train_dataset)\r\n",
        "print(\"Valid\", validation_dataset)\r\n",
        "print(\"Challenge\", challenge_dataset)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset({\n",
            "    features: ['id', 'context_id', 'question_id', 'domain', 'metadata', 'context', 'question', 'question_type', 'answers', 'correct_answer_id'],\n",
            "    num_rows: 10246\n",
            "})\n",
            "Valid Dataset({\n",
            "    features: ['id', 'context_id', 'question_id', 'domain', 'metadata', 'context', 'question', 'question_type', 'answers', 'correct_answer_id'],\n",
            "    num_rows: 2164\n",
            "})\n",
            "Challenge Dataset({\n",
            "    features: ['id', 'context_id', 'question_id', 'domain', 'metadata', 'context', 'question', 'question_type', 'answers', 'correct_answer_id'],\n",
            "    num_rows: 556\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfFMoi3qmOoD",
        "outputId": "e3f560c2-46a4-4408-d535-d8ee6e388b49"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answers': ['not enough information',\n",
              "  'to visit family',\n",
              "  'parents had problems',\n",
              "  'for tutoring'],\n",
              " 'context': \"That fall came and I went back to Michigan and the school year went by and summer came and I never really thought about it. I'm not even sure if I was officially asked, I just wound up heading back to New Jersey when school was out. I think my parents thought it was a good enough deal. They were already having some problems and without Nonna there anymore to take care of me I think my cousin's house on the coast seemed like as good a spot as any to stick me for the summer. It certainly wasn't because of any great love between me and my cousin. We weren't really very good friends at that point. I think she saw me as sort of foisted off on her and getting in the way of her summers. Which was a fair enough judgment. But she could have been nicer. It's pretty amazing that she wound up as my Maid of Honor. Time does strange things. Your lovable jack-ass of a father would mention something about magic in here. You know if you took a group of fifty strangers, had them chat with your father for half an hour then with me for half an hour, then told them that one of us was an English Professor and one of us was head of distribution in the northeast for a large soft drink manufacturing concern, I'm pretty sure all fifty would peg your father as the English Professor and me as the head of distribution. He's honestly so good at what he does that I can almost allow him to claim it's magic except that it'd be nice if he took credit for some of the things he's done with his life. Of course he has this idea that he deserves credit for all sorts of things that he had no control over. Like our first kiss.\",\n",
              " 'context_id': 'f001',\n",
              " 'correct_answer_id': 3,\n",
              " 'domain': 'fiction',\n",
              " 'id': 'f001_0',\n",
              " 'metadata': {'author': 'Joseph Devon',\n",
              "  'title': 'Black Eyed Susan',\n",
              "  'url': 'http://manybooks.net/pages/devonjother08black_eyed_susan/0.html'},\n",
              " 'question': 'Why was this character sent away after each school year?',\n",
              " 'question_id': '0',\n",
              " 'question_type': 'Causality'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXDBq9akyjCP",
        "outputId": "38da871b-4c3d-474a-bec1-111813027f69"
      },
      "source": [
        "# Process the dataset - add a column with the length of the context texts\r\n",
        "train_dataset = train_dataset.map(lambda x: {\"context_length\": len(x[\"context\"].split())})\r\n",
        "train_dataset = train_dataset.map(lambda x: {\"question_length\": len(x[\"question\"].split())})\r\n",
        "train_dataset = train_dataset.map(lambda x: {\"number_of_answer\": len(x[\"answers\"])})"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/quail/quail/1.3.0/17ee3da60d3c4939a1679994e241f120c7c007baee8cbfb226baf6e106095101/cache-74c43f36c1c4b703.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/quail/quail/1.3.0/17ee3da60d3c4939a1679994e241f120c7c007baee8cbfb226baf6e106095101/cache-f3319e3bd92e0fad.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/quail/quail/1.3.0/17ee3da60d3c4939a1679994e241f120c7c007baee8cbfb226baf6e106095101/cache-aa0e8f8d0b918dc5.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEXfZbutzyiX"
      },
      "source": [
        "import pandas as pd\r\n",
        "df = pd.DataFrame()"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE8znbeoysQj"
      },
      "source": [
        "context_length = [train_dataset[i]['context_length'] for i in range(len(train_dataset))]\r\n",
        "number_of_answer = [train_dataset[i]['number_of_answer'] for i in range(len(train_dataset))]\r\n",
        "question_length = [train_dataset[i]['question_length'] for i in range(len(train_dataset))]\r\n",
        "question_type = [train_dataset[i]['question_type'] for i in range(len(train_dataset))]\r\n",
        "domain_type = [train_dataset[i]['domain'] for i in range(len(train_dataset))]\r\n",
        "df['context_length']=context_length\r\n",
        "df['number_of_answer']=number_of_answer\r\n",
        "df['question_length']=question_length\r\n",
        "df['domain_type']=domain_type\r\n",
        "df['question_type']=question_type"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KQrOmAX0N0v"
      },
      "source": [
        "### 2. Check the number of answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV8vlmHV0Cd3",
        "outputId": "19dcc6bd-1565-498a-8cb6-f7cb4fe27f8b"
      },
      "source": [
        "df.number_of_answer.value_counts()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    10246\n",
              "Name: number_of_answer, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAScihK90U_C"
      },
      "source": [
        "### 3. Check Number of words in the context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "xHTQ7DIt0TDQ",
        "outputId": "d3faaace-003d-402c-ce6a-ca3bc8f6ebab"
      },
      "source": [
        "df.context_length.hist()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f88172cfcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiFFytFF02lU"
      },
      "source": [
        "### 4. Check Number of words in the answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "R2nA-oHm09ja",
        "outputId": "d6e3c76a-7313-47a3-8975-dfcd638ba45c"
      },
      "source": [
        "df.question_length.hist()"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f88195415c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZHdsyJV4Pp5",
        "outputId": "da6c62be-d889-4ab3-947e-4beb0a458b85"
      },
      "source": [
        "df.question_length.value_counts()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7     1438\n",
              "8     1246\n",
              "6     1161\n",
              "9     1098\n",
              "5      961\n",
              "10     951\n",
              "11     692\n",
              "4      672\n",
              "12     498\n",
              "13     374\n",
              "14     276\n",
              "3      231\n",
              "15     181\n",
              "16     121\n",
              "17     114\n",
              "18      60\n",
              "19      59\n",
              "20      30\n",
              "2       20\n",
              "21      20\n",
              "22      15\n",
              "23       7\n",
              "24       6\n",
              "25       5\n",
              "1        3\n",
              "26       2\n",
              "29       2\n",
              "27       2\n",
              "28       1\n",
              "Name: question_length, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdXAxxLE3dHY"
      },
      "source": [
        "### 5. Different Types of Questions Available "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdsL_Wh33V7N",
        "outputId": "4593b99a-0560-4a06-fa83-d849af171d2f"
      },
      "source": [
        "df.question_type.value_counts()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Factual               1185\n",
              "Unanswerable          1151\n",
              "Character_identity    1137\n",
              "Entity_properties     1137\n",
              "Causality             1135\n",
              "Temporal_order        1131\n",
              "Event_duration        1124\n",
              "Subsequent_state      1124\n",
              "Belief_states         1122\n",
              "Name: question_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VZyH3I55Aiu"
      },
      "source": [
        "### 6. Different Type of Domain Available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_F0ZZ9B4__g",
        "outputId": "9db40a31-0871-44ca-dc68-59c6b5519452"
      },
      "source": [
        "df.domain_type.value_counts()"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fiction         2686\n",
              "blogs           2520\n",
              "user_stories    2520\n",
              "news            2520\n",
              "Name: domain_type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XXJlqHH1CVT"
      },
      "source": [
        "## Task1 Observations: \r\n",
        "* Number of words in the question is in range (2, 27) mostly and context is having up to 400 words. We might not need to use sliding window here inorder to get resonable performance\r\n",
        "* The Dataset is having `3 one word question` and `20 two word question`, that might seems a bit weird.\r\n",
        "* We have four choices of answers\r\n",
        "* Our Dataset is having `Four Domain` and `9 Type of Questions` and the distribution of all this type is fairly even"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSm-y5nDr0p7"
      },
      "source": [
        "## Task2. Dataset Preprocessing\r\n",
        "### 1. Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqgft1AaogDv",
        "outputId": "7db5b265-cd3e-4785-e12f-6dc857ab917c"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\r\n",
        "model_name = \"allenai/unifiedqa-t5-large\" # you can specify the model size here\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
        "model =  AutoModelWithLMHead.from_pretrained(model_name)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:821: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x27mon0sAC-"
      },
      "source": [
        "def run_model(input_string, **generator_args):\r\n",
        "  input_dict = tokenizer(input_string, max_length=512,  truncation=True, return_tensors=\"pt\")\r\n",
        "  input_ids = input_dict['input_ids'].to(device)\r\n",
        "  attention_mask = input_dict['attention_mask'].to(device)\r\n",
        "  res = model.generate(input_ids=input_ids, attention_mask=attention_mask, **generator_args)\r\n",
        "  return [tokenizer.decode(x) for x in res]"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwMBOShmtU4-",
        "outputId": "668cc659-e251-45a1-ef35-8815d2ad0596"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 1024)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (17): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (18): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (20): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (21): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (23): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 1024)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 16)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (17): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (18): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (20): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (21): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (23): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
              "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbayjqjAsrkK"
      },
      "source": [
        "### 2. Data Preprocessing\r\n",
        "\r\n",
        "out dataset is like MCTest (Multiple-choice QA) used for training of UnifiedQA\r\n",
        "We need to pass the data in `question \\n (A) Choice (B) Choice \\n context ` according to the [Unified Github](https://github.com/allenai/unifiedqa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdG1AvyxsNFc"
      },
      "source": [
        "def input_target_creator(dataset):\r\n",
        "  inputs_features = []\r\n",
        "  target = []\r\n",
        "  for i in range(len(dataset)):\r\n",
        "    question =  dataset[i]['question']\r\n",
        "    answers = dataset[i]['answers']\r\n",
        "    context = dataset[i]['context']\r\n",
        "    correct_answer = dataset[i]['answers'][dataset[i]['correct_answer_id']-1]\r\n",
        "    target.append(correct_answer)\r\n",
        "    input = question + \" \\\\n\" + \" (A) \" + answers[0] + \" (B) \" + answers[1] + \" (C) \" + answers[2] + \" (D) \" + answers[3]  + \" \\\\n \"  + context\r\n",
        "    inputs_features.append(input)\r\n",
        "  return inputs_features, target"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txu3Mc2hsxwM"
      },
      "source": [
        "inputs_features, target = input_target_creator(validation_dataset)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqCd6pICE_cV"
      },
      "source": [
        "new_df = pd.DataFrame()\r\n",
        "new_df['input_features'] = inputs_features\r\n",
        "new_df['target'] = target"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmhkXEUJoXnh"
      },
      "source": [
        "new_df['feature_length'] = new_df['input_features'].apply(lambda input: len(tokenizer.tokenize(input)))"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjEIJgBXp76i"
      },
      "source": [
        "# remove data with more than 512 length\r\n",
        "new_df = new_df.query('feature_length<=512')"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9eea2f96205d498a8a4cf0ec7d006a0d",
            "a48c8f21e0544e5f8a573644ae89972a",
            "1c259cd435b24cff8187ec0f93c90c8b",
            "ba7932c13c054be0a57da2aeac45e909",
            "b811845c269e45079051ec27aeb0644c",
            "e7582f4a7c2d4585ba40c4c586cff9ed",
            "f9122f0c302b4574864191ca530c2bbf",
            "a52c3d0b594f4946a8f416d9773b42ff"
          ]
        },
        "id": "TxR_67d_FXtH",
        "outputId": "fe601330-9952-404d-dcc0-53f620de6539"
      },
      "source": [
        "new_df['prediction'] = new_df['input_features'].progress_apply(run_model)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9eea2f96205d498a8a4cf0ec7d006a0d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1276.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rmzBjqvFwQO"
      },
      "source": [
        "\"\"\"Evaluation utilities.\"\"\"\r\n",
        "import argparse\r\n",
        "import collections\r\n",
        "import json\r\n",
        "import os\r\n",
        "import re\r\n",
        "import string\r\n",
        "import sys\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def normalize_answer(s):\r\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\r\n",
        "    def remove_articles(text):\r\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\r\n",
        "        return re.sub(regex, ' ', text)\r\n",
        "    def white_space_fix(text):\r\n",
        "        return ' '.join(text.split())\r\n",
        "    def remove_punc(text):\r\n",
        "        exclude = set(string.punctuation)\r\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\r\n",
        "    def lower(text):\r\n",
        "        return text.lower()\r\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\r\n",
        "\r\n",
        "\r\n",
        "def get_tokens(s):\r\n",
        "    if not s:\r\n",
        "        return []\r\n",
        "    return normalize_answer(s).split()\r\n",
        "\r\n",
        "\r\n",
        "def compute_exact(a_gold, a_pred):\r\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\r\n",
        "\r\n",
        "\r\n",
        "def compute_f1(a_gold, a_pred):\r\n",
        "    gold_toks = get_tokens(a_gold)\r\n",
        "    pred_toks = get_tokens(a_pred)\r\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\r\n",
        "    num_same = sum(common.values())\r\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\r\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\r\n",
        "        return int(gold_toks == pred_toks)\r\n",
        "    if num_same == 0:\r\n",
        "        return 0\r\n",
        "    precision = 1.0 * num_same / len(pred_toks)\r\n",
        "    recall = 1.0 * num_same / len(gold_toks)\r\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\r\n",
        "    return f1\r\n",
        "\r\n",
        "def make_eval_dict(exact_scores, f1_scores):\r\n",
        "    total = len(exact_scores)\r\n",
        "    return collections.OrderedDict([(\"exact\", 100.0 * sum(exact_scores.values / total)), (\"f1\", 100.0 * sum(f1_scores.values / total)),(\"total\", total)])"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUfF4ZWpb-LW"
      },
      "source": [
        "new_df['new_prediction'] = new_df['prediction'].apply(lambda predi : predi[0])"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "n-Y82JswdBJr",
        "outputId": "2e158c58-b5d9-4148-f0c9-010ae885a88e"
      },
      "source": [
        "new_df.head()"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_features</th>\n",
              "      <th>target</th>\n",
              "      <th>feature_length</th>\n",
              "      <th>prediction</th>\n",
              "      <th>exact_match</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>new_prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How long was Candy trying to seduce Larry? \\n ...</td>\n",
              "      <td>All day</td>\n",
              "      <td>481</td>\n",
              "      <td>[about 2 hours]</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>about 2 hours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is probably true about Larry. \\n (A) Larr...</td>\n",
              "      <td>not enough information</td>\n",
              "      <td>499</td>\n",
              "      <td>[not enough information]</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>not enough information</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What was Larry impressed with? \\n (A) How wind...</td>\n",
              "      <td>not enough information</td>\n",
              "      <td>484</td>\n",
              "      <td>[That Candy had all her teeth]</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>That Candy had all her teeth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why did Candy press her legs together. \\n (A) ...</td>\n",
              "      <td>To warm her butt.</td>\n",
              "      <td>486</td>\n",
              "      <td>[because she was cold]</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>because she was cold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who thought prostitutes were revolting? \\n (A)...</td>\n",
              "      <td>not enough information</td>\n",
              "      <td>479</td>\n",
              "      <td>[Larry]</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Larry</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      input_features  ...                new_prediction\n",
              "0  How long was Candy trying to seduce Larry? \\n ...  ...                 about 2 hours\n",
              "1  What is probably true about Larry. \\n (A) Larr...  ...        not enough information\n",
              "2  What was Larry impressed with? \\n (A) How wind...  ...  That Candy had all her teeth\n",
              "3  Why did Candy press her legs together. \\n (A) ...  ...          because she was cold\n",
              "4  Who thought prostitutes were revolting? \\n (A)...  ...                         Larry\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Os3RqlcTDU",
        "outputId": "71e76ee6-6ffb-4141-f2b7-ae93adcc287a"
      },
      "source": [
        "new_df['exact_match'] = new_df.apply(lambda row:compute_exact(row['target'], row['new_prediction']), axis=1)\r\n",
        "new_df['f1_score'] = new_df.apply(lambda row:compute_f1(row['target'], row['new_prediction']), axis=1)\r\n",
        "make_eval_dict(new_df['exact_match'], new_df['f1_score'])"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('exact', 12.460815047021967),\n",
              "             ('f1', 22.75461298340418),\n",
              "             ('total', 1276)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diCKtFPRcsCk"
      },
      "source": [
        "UnifiefQA_Small:\r\n",
        "```\r\n",
        "OrderedDict([('exact', 21.63009404388722),\r\n",
        "             ('f1', 27.687265700335846),\r\n",
        "             ('total', 1276)])\r\n",
        "```\r\n",
        "\r\n",
        "UnifiedQA_Base:\r\n",
        "```\r\n",
        "OrderedDict([('exact', 14.576802507837025),\r\n",
        "             ('f1', 23.737297181234666),\r\n",
        "             ('total', 1276)])\r\n",
        "```\r\n",
        "\r\n",
        "UnifiedQA-Large:\r\n",
        "```\r\n",
        "OrderedDict([('exact', 12.460815047021967),\r\n",
        "             ('f1', 22.75461298340418),\r\n",
        "             ('total', 1276)])\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLtc7DDKdHYE"
      },
      "source": [
        ""
      ],
      "execution_count": 188,
      "outputs": []
    }
  ]
}