{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CheckingTFScripts.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFx7tnxEF7CBPR6K+BUjIQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/UnderstandingNLP/blob/master/CheckingTFScripts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYmlQiIUtS4O"
      },
      "source": [
        "## Install Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1aipZH0tEvW",
        "outputId": "5010354a-a7a3-4107-f9da-29c07c397401"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\r\n",
        "%cd transformers\r\n",
        "!pip install .\r\n",
        "!pip install -r ./examples/multiple-choice/requirements.txt\r\n",
        "%cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 65438 (delta 3), reused 2 (delta 0), pack-reused 65430\u001b[K\n",
            "Receiving objects: 100% (65438/65438), 48.94 MiB | 24.44 MiB/s, done.\n",
            "Resolving deltas: 100% (46407/46407), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (3.7.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (2020.12.5)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.4.0.dev0-cp37-none-any.whl size=1896183 sha256=21a8d7f8af032853685b00fdf131bd90b13620ae38f1ef412a2acb2595727d50\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9npk0cb2/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=f0f354abe90a646f5fdfb0bcd06eb2c0296b6f35d2b174ff69cc109d800d46bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.0.dev0\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r ./examples/multiple-choice/requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r ./examples/multiple-choice/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->-r ./examples/multiple-choice/requirements.txt (line 2)) (54.0.0)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNAHYedQtXru"
      },
      "source": [
        "## Get data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "def40PRQtWWE",
        "outputId": "1b58dc8a-a7cc-4235-e492-fdfb19353164"
      },
      "source": [
        "!mkdir swag\r\n",
        "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/test.csv -O swag/test.csv\r\n",
        "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/train.csv -O swag/train.csv\r\n",
        "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/val.csv -O swag/val.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-05 09:26:46--  https://raw.githubusercontent.com/rowanz/swagaf/master/data/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7817885 (7.5M) [text/plain]\n",
            "Saving to: ‘swag/test.csv’\n",
            "\n",
            "swag/test.csv       100%[===================>]   7.46M  26.7MB/s    in 0.3s    \n",
            "\n",
            "2021-03-05 09:26:46 (26.7 MB/s) - ‘swag/test.csv’ saved [7817885/7817885]\n",
            "\n",
            "--2021-03-05 09:26:46--  https://raw.githubusercontent.com/rowanz/swagaf/master/data/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28243333 (27M) [text/plain]\n",
            "Saving to: ‘swag/train.csv’\n",
            "\n",
            "swag/train.csv      100%[===================>]  26.93M  51.3MB/s    in 0.5s    \n",
            "\n",
            "2021-03-05 09:26:47 (51.3 MB/s) - ‘swag/train.csv’ saved [28243333/28243333]\n",
            "\n",
            "--2021-03-05 09:26:47--  https://raw.githubusercontent.com/rowanz/swagaf/master/data/val.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7893588 (7.5M) [text/plain]\n",
            "Saving to: ‘swag/val.csv’\n",
            "\n",
            "swag/val.csv        100%[===================>]   7.53M  28.3MB/s    in 0.3s    \n",
            "\n",
            "2021-03-05 09:26:48 (28.3 MB/s) - ‘swag/val.csv’ saved [7893588/7893588]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw0WJWBWtg70"
      },
      "source": [
        "## Run Script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3J7jpe8tfaO",
        "outputId": "7947504d-1cae-4084-f7ad-b222fec46fa9"
      },
      "source": [
        "!python ./transformers/examples/multiple-choice/run_tf_multiple_choice.py \\\r\n",
        "--task_name swag \\\r\n",
        "--model_name_or_path bert-base-cased \\\r\n",
        "--do_train \\\r\n",
        "--do_eval \\\r\n",
        "--data_dir ./swag \\\r\n",
        "--learning_rate 5e-5 \\\r\n",
        "--num_train_epochs 3 \\\r\n",
        "--max_seq_length 80 \\\r\n",
        "--output_dir models_bert/swag_base \\\r\n",
        "--per_gpu_eval_batch_size=16 \\\r\n",
        "--per_device_train_batch_size=16 \\\r\n",
        "--gradient_accumulation_steps 2 \\\r\n",
        "--overwrite_output"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-05 10:12:41.516514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "[INFO|training_args.py:618] 2021-03-05 10:12:43,082 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:542] 2021-03-05 10:12:43,095 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "[INFO|training_args_tf.py:192] 2021-03-05 10:12:43,096 >> Tensorflow: setting up strategy\n",
            "2021-03-05 10:12:43.096682: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-05 10:12:43.098246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-05 10:12:43.108696: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-03-05 10:12:43.108744: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0adf80b6c3dc): /proc/driver/nvidia/version does not exist\n",
            "2021-03-05 10:12:43.109545: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "03/05/2021 10:12:43 - WARNING - __main__ -   device: cpu, n_replicas: 1, 16-bits training: False\n",
            "03/05/2021 10:12:43 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='models_bert/swag_base', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=16, gradient_accumulation_steps=2, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Mar05_10-12-43_0adf80b6c3dc', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='models_bert/swag_base', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, tpu_name=None, tpu_zone=None, gcp_project=None, poly_power=1.0, xla=False)\n",
            "03/05/2021 10:12:43 - INFO - filelock -   Lock 139642859190608 acquired on /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655.lock\n",
            "[INFO|file_utils.py:1327] 2021-03-05 10:12:43,172 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpeydjhkio\n",
            "Downloading: 100% 433/433 [00:00<00:00, 353kB/s]\n",
            "[INFO|file_utils.py:1331] 2021-03-05 10:12:43,231 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "[INFO|file_utils.py:1334] 2021-03-05 10:12:43,231 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "03/05/2021 10:12:43 - INFO - filelock -   Lock 139642859190608 released on /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655.lock\n",
            "[INFO|configuration_utils.py:459] 2021-03-05 10:12:43,232 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "[INFO|configuration_utils.py:495] 2021-03-05 10:12:43,233 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"swag\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:459] 2021-03-05 10:12:43,289 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.0d87139f53a477d9f900f8a9020c367863079014bdaf2aa713f4b64cf1782655\n",
            "[INFO|configuration_utils.py:495] 2021-03-05 10:12:43,290 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "03/05/2021 10:12:43 - INFO - filelock -   Lock 139642859189648 acquired on /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\n",
            "[INFO|file_utils.py:1327] 2021-03-05 10:12:43,443 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf60i067z\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 4.44MB/s]\n",
            "[INFO|file_utils.py:1331] 2021-03-05 10:12:43,548 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|file_utils.py:1334] 2021-03-05 10:12:43,548 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "03/05/2021 10:12:43 - INFO - filelock -   Lock 139642859189648 released on /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791.lock\n",
            "03/05/2021 10:12:43 - INFO - filelock -   Lock 139642859189648 acquired on /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\n",
            "[INFO|file_utils.py:1327] 2021-03-05 10:12:43,607 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpq9fgwakh\n",
            "Downloading: 100% 436k/436k [00:00<00:00, 7.40MB/s]\n",
            "[INFO|file_utils.py:1331] 2021-03-05 10:12:43,724 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "[INFO|file_utils.py:1334] 2021-03-05 10:12:43,724 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "03/05/2021 10:12:43 - INFO - filelock -   Lock 139642859189648 released on /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6.lock\n",
            "[INFO|tokenization_utils_base.py:1716] 2021-03-05 10:12:43,725 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "[INFO|tokenization_utils_base.py:1716] 2021-03-05 10:12:43,725 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
            "03/05/2021 10:12:43 - INFO - filelock -   Lock 139642794672912 acquired on /root/.cache/huggingface/transformers/01800f4158e284e2447020e0124bc3f6aea3ac49848e744594f7cce8ee5ac0a4.a7137b2090d9302d722735af604b4c142ec9d1bfc31be7cbbe230aea9d5cfb76.h5.lock\n",
            "[INFO|file_utils.py:1327] 2021-03-05 10:12:43,814 >> https://huggingface.co/bert-base-cased/resolve/main/tf_model.h5 not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcoby21f2\n",
            "Downloading: 100% 527M/527M [00:09<00:00, 55.4MB/s]\n",
            "[INFO|file_utils.py:1331] 2021-03-05 10:12:53,563 >> storing https://huggingface.co/bert-base-cased/resolve/main/tf_model.h5 in cache at /root/.cache/huggingface/transformers/01800f4158e284e2447020e0124bc3f6aea3ac49848e744594f7cce8ee5ac0a4.a7137b2090d9302d722735af604b4c142ec9d1bfc31be7cbbe230aea9d5cfb76.h5\n",
            "[INFO|file_utils.py:1334] 2021-03-05 10:12:53,563 >> creating metadata file for /root/.cache/huggingface/transformers/01800f4158e284e2447020e0124bc3f6aea3ac49848e744594f7cce8ee5ac0a4.a7137b2090d9302d722735af604b4c142ec9d1bfc31be7cbbe230aea9d5cfb76.h5\n",
            "03/05/2021 10:12:53 - INFO - filelock -   Lock 139642794672912 released on /root/.cache/huggingface/transformers/01800f4158e284e2447020e0124bc3f6aea3ac49848e744594f7cce8ee5ac0a4.a7137b2090d9302d722735af604b4c142ec9d1bfc31be7cbbe230aea9d5cfb76.h5.lock\n",
            "[INFO|modeling_tf_utils.py:1222] 2021-03-05 10:12:53,563 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/01800f4158e284e2447020e0124bc3f6aea3ac49848e744594f7cce8ee5ac0a4.a7137b2090d9302d722735af604b4c142ec9d1bfc31be7cbbe230aea9d5cfb76.h5\n",
            "[WARNING|modeling_tf_utils.py:1270] 2021-03-05 10:12:57,539 >> All model checkpoint layers were used when initializing TFBertForMultipleChoice.\n",
            "\n",
            "[WARNING|modeling_tf_utils.py:1274] 2021-03-05 10:12:57,539 >> Some layers of TFBertForMultipleChoice were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "03/05/2021 10:12:57 - INFO - utils_multiple_choice -   Creating features from dataset file at ./swag\n",
            "03/05/2021 10:12:57 - INFO - utils_multiple_choice -   LOOKING AT ./swag train\n",
            "03/05/2021 10:12:58 - INFO - utils_multiple_choice -   Training examples: 73546\n",
            "convert examples to features: 0it [00:00, ?it/s]03/05/2021 10:12:58 - INFO - utils_multiple_choice -   Writing example 0 of 73546\n",
            "convert examples to features: 9890it [00:06, 1643.46it/s]03/05/2021 10:13:05 - INFO - utils_multiple_choice -   Writing example 10000 of 73546\n",
            "convert examples to features: 19972it [00:12, 1623.26it/s]03/05/2021 10:13:11 - INFO - utils_multiple_choice -   Writing example 20000 of 73546\n",
            "convert examples to features: 29976it [00:19, 1673.29it/s]03/05/2021 10:13:18 - INFO - utils_multiple_choice -   Writing example 30000 of 73546\n",
            "convert examples to features: 39990it [00:26, 1646.10it/s]03/05/2021 10:13:24 - INFO - utils_multiple_choice -   Writing example 40000 of 73546\n",
            "convert examples to features: 49877it [00:32, 1031.47it/s]03/05/2021 10:13:31 - INFO - utils_multiple_choice -   Writing example 50000 of 73546\n",
            "convert examples to features: 59896it [00:39, 1604.74it/s]03/05/2021 10:13:37 - INFO - utils_multiple_choice -   Writing example 60000 of 73546\n",
            "convert examples to features: 69983it [00:46, 1591.74it/s]03/05/2021 10:13:44 - INFO - utils_multiple_choice -   Writing example 70000 of 73546\n",
            "convert examples to features: 73546it [00:48, 1522.16it/s]\n",
            "03/05/2021 10:13:47 - INFO - utils_multiple_choice -   *** Example ***\n",
            "03/05/2021 10:13:47 - INFO - utils_multiple_choice -   feature: InputFeatures(example_id='3416', input_ids=[[[101, 7667, 1104, 1103, 16018, 2647, 1205, 1103, 2472, 2355, 1353, 9621, 10488, 5349, 119, 102, 138, 7505, 1413, 4488, 1118, 3179, 1205, 1103, 2472, 1773, 1147, 5349, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 7667, 1104, 1103, 16018, 2647, 1205, 1103, 2472, 2355, 1353, 9621, 10488, 5349, 119, 102, 138, 7505, 1413, 1144, 1767, 8320, 1172, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 7667, 1104, 1103, 16018, 2647, 1205, 1103, 2472, 2355, 1353, 9621, 10488, 5349, 119, 102, 138, 7505, 1413, 8121, 1105, 1152, 112, 1231, 1796, 5923, 1105, 6153, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 7667, 1104, 1103, 16018, 2647, 1205, 1103, 2472, 2355, 1353, 9621, 10488, 5349, 119, 102, 138, 7505, 1413, 3587, 1103, 1730, 2483, 12642, 1103, 2099, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], attention_mask=[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], token_type_ids=[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], label=0)\n",
            "03/05/2021 10:13:47 - INFO - utils_multiple_choice -   *** Example ***\n",
            "03/05/2021 10:13:47 - INFO - utils_multiple_choice -   feature: InputFeatures(example_id='3417', input_ids=[[[101, 138, 7505, 1413, 4488, 1118, 3179, 1205, 1103, 2472, 1773, 1147, 5349, 119, 102, 7667, 1104, 1103, 16018, 1132, 1773, 10473, 1403, 185, 4553, 1105, 14118, 1141, 1286, 1296, 1107, 3613, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 138, 7505, 1413, 4488, 1118, 3179, 1205, 1103, 2472, 1773, 1147, 5349, 119, 102, 7667, 1104, 1103, 16018, 3074, 2494, 2019, 1103, 22943, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 138, 7505, 1413, 4488, 1118, 3179, 1205, 1103, 2472, 1773, 1147, 5349, 119, 102, 7667, 1104, 1103, 16018, 3430, 1106, 1505, 1112, 1218, 1373, 1103, 3515, 1373, 1114, 1103, 1467, 1217, 10361, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 138, 7505, 1413, 4488, 1118, 3179, 1205, 1103, 2472, 1773, 1147, 5349, 119, 102, 7667, 1104, 1103, 16018, 2760, 1106, 1505, 14058, 117, 9455, 28107, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], attention_mask=[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], token_type_ids=[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], label=3)\n",
            "03/05/2021 10:13:47 - INFO - utils_multiple_choice -   Creating features from dataset file at ./swag\n",
            "03/05/2021 10:13:47 - INFO - utils_multiple_choice -   LOOKING AT ./swag dev\n",
            "03/05/2021 10:13:47 - INFO - utils_multiple_choice -   Training examples: 20006\n",
            "convert examples to features: 0it [00:00, ?it/s]03/05/2021 10:13:47 - INFO - utils_multiple_choice -   Writing example 0 of 20006\n",
            "convert examples to features: 9867it [00:06, 1541.25it/s]03/05/2021 10:13:53 - INFO - utils_multiple_choice -   Writing example 10000 of 20006\n",
            "convert examples to features: 19897it [00:14, 1582.82it/s]03/05/2021 10:14:01 - INFO - utils_multiple_choice -   Writing example 20000 of 20006\n",
            "convert examples to features: 20006it [00:14, 1374.32it/s]\n",
            "03/05/2021 10:14:01 - INFO - utils_multiple_choice -   *** Example ***\n",
            "03/05/2021 10:14:01 - INFO - utils_multiple_choice -   feature: InputFeatures(example_id='18313', input_ids=[[[101, 6510, 2211, 1147, 1257, 12938, 119, 102, 1153, 26227, 1116, 1123, 2342, 117, 1173, 21718, 8355, 1468, 1755, 1800, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 6510, 2211, 1147, 1257, 12938, 119, 102, 1153, 3587, 1114, 1160, 1651, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 6510, 2211, 1147, 1257, 12938, 119, 102, 1153, 7104, 2494, 2019, 1800, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 6510, 2211, 1147, 1257, 12938, 119, 102, 1153, 8089, 1213, 1112, 1123, 3676, 17209, 1116, 1149, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], attention_mask=[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], token_type_ids=[[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], label=2)\n",
            "03/05/2021 10:14:01 - INFO - utils_multiple_choice -   *** Example ***\n",
            "03/05/2021 10:14:01 - INFO - utils_multiple_choice -   feature: InputFeatures(example_id='18419', input_ids=[[[101, 1124, 13863, 1103, 9580, 1205, 1103, 2885, 1105, 1154, 1103, 7605, 119, 102, 1124, 2736, 1120, 170, 5220, 1107, 1103, 5220, 1112, 1119, 12642, 1800, 2647, 1194, 170, 1442, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 1124, 13863, 1103, 9580, 1205, 1103, 2885, 1105, 1154, 1103, 7605, 119, 102, 1124, 6260, 117, 5578, 1106, 170, 4355, 1104, 3538, 1114, 1103, 8808, 1590, 117, 1150, 112, 188, 2288, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 1124, 13863, 1103, 9580, 1205, 1103, 2885, 1105, 1154, 1103, 7605, 119, 102, 1124, 18387, 1103, 1459, 1105, 13863, 1103, 9580, 1154, 170, 14330, 1187, 1119, 10383, 1317, 13270, 1112, 1234, 2824, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[101, 1124, 13863, 1103, 9580, 1205, 1103, 2885, 1105, 1154, 1103, 7605, 119, 102, 1124, 7561, 1103, 3821, 1149, 1104, 1117, 4480, 1105, 1493, 1122, 1106, 1800, 112, 188, 5372, 1918, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], attention_mask=[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], token_type_ids=[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], label=2)\n",
            "[INFO|trainer_tf.py:117] 2021-03-05 10:14:01,956 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "[INFO|trainer_tf.py:125] 2021-03-05 10:14:01,957 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n",
            "2021-03-05 10:14:01.963991: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-05 10:14:01.964579: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4396\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:528] 2021-03-05 10:14:01,971 >> ***** Running training *****\n",
            "[INFO|trainer_tf.py:529] 2021-03-05 10:14:01,971 >>   Num examples = 73546\n",
            "[INFO|trainer_tf.py:531] 2021-03-05 10:14:01,971 >>   Num Epochs = 3\n",
            "[INFO|trainer_tf.py:532] 2021-03-05 10:14:01,971 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer_tf.py:534] 2021-03-05 10:14:01,971 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer_tf.py:536] 2021-03-05 10:14:01,971 >>   Gradient Accumulation steps = 2\n",
            "[INFO|trainer_tf.py:537] 2021-03-05 10:14:01,971 >>   Steps per epoch = 2299\n",
            "[INFO|trainer_tf.py:538] 2021-03-05 10:14:01,971 >>   Total optimization steps = 6897\n",
            "2021-03-05 10:14:02.023391: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-03-05 10:14:02.023847: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz\n",
            "convert examples to features: 0it [00:00, ?it/s]03/05/2021 10:14:02 - INFO - utils_multiple_choice -   Writing example 0 of 73546\n",
            "2021-03-05 10:14:02.052628: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 938, in generator_py_func\n",
            "    (ret_array.shape, expected_shape))\n",
            "\n",
            "ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\", line 2113, in execution_mode\n",
            "2021-03-05 10:14:02.054051: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 938, in generator_py_func\n",
            "    (ret_array.shape, expected_shape))\n",
            "\n",
            "ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "\n",
            "\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 733, in _next_internal\n",
            "2021-03-05 10:14:02.055521: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 938, in generator_py_func\n",
            "    (ret_array.shape, expected_shape))\n",
            "\n",
            "ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "\n",
            "\n",
            "2021-03-05 10:14:02.056377: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 938, in generator_py_func\n",
            "    (ret_array.shape, expected_shape))\n",
            "\n",
            "ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "\n",
            "\n",
            "    output_shapes=self._flat_output_shapes)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2579, in iterator_get_next\n",
            "2021-03-05 10:14:02.057092: W tensorflow/core/framework/op_kernel.cc:1751] Invalid argument: ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 938, in generator_py_func\n",
            "    (ret_array.shape, expected_shape))\n",
            "\n",
            "ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "\n",
            "\n",
            "    _ops.raise_from_not_ok_status(e, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\n",
            "    six.raise_from(core._status_to_exception(e.code, message), None)\n",
            "  File \"<string>\", line 3, in raise_from\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 938, in generator_py_func\n",
            "    (ret_array.shape, expected_shape))\n",
            "\n",
            "ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "\n",
            "\n",
            "\t [[{{node PyFunc}}]]\n",
            "\t [[MultiDeviceIteratorGetNextFromShard]]\n",
            "\t [[RemoteCall]] [Op:IteratorGetNext]\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"./transformers/examples/multiple-choice/run_tf_multiple_choice.py\", line 218, in <module>\n",
            "    main()\n",
            "  File \"./transformers/examples/multiple-choice/run_tf_multiple_choice.py\", line 199, in main\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer_tf.py\", line 548, in train\n",
            "    for step, batch in enumerate(train_ds):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 632, in __next__\n",
            "    return self.get_next()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 663, in get_next\n",
            "    self._iterators[i].get_next_as_list_static_shapes(new_name))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1619, in get_next_as_list_static_shapes\n",
            "    return self._format_data_list_with_options(self._iterator.get_next())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 585, in get_next\n",
            "    result.append(self._device_iterators[i].get_next())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 800, in get_next\n",
            "    return self._next_internal()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 739, in _next_internal\n",
            "    return structure.from_compatible_tensor_list(self._element_spec, ret)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\", line 2116, in execution_mode\n",
            "    executor_new.wait()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/executor.py\", line 69, in wait\n",
            "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 938, in generator_py_func\n",
            "    (ret_array.shape, expected_shape))\n",
            "\n",
            "ValueError: `generator` yielded an element of shape (4, 1, 80) where an element of shape (None, None) was expected.\n",
            "\n",
            "\n",
            "\t [[{{node PyFunc}}]]\n",
            "\t [[MultiDeviceIteratorGetNextFromShard]]\n",
            "\t [[RemoteCall]]\n",
            "2021-03-05 10:14:04.296184: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\n",
            "\t [[{{node PyFunc}}]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpcN47Katmbs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}