{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CheckingDeepspeed.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMoPHtK3OGtxEVhqjhkLOMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/UnderstandingNLP/blob/master/CheckingDeepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhkveNAMO7L2"
      },
      "source": [
        "We want to check slowdown in while using fp16 instead fp32 while evaluation step.\n",
        "https://github.com/huggingface/transformers/issues/10816"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUrId8TSLHhS",
        "outputId": "cfad944a-cb19-434a-9077-ea09da530656"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "%cd transformers\n",
        "!pip install .\n",
        "!pip install sacrebleu\n",
        "!pip install -q sentencepiece\n",
        "!pip install -r ./examples/question-answering/requirements.txt\n",
        "%cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 68149 (delta 26), reused 44 (delta 10), pack-reused 68070\u001b[K\n",
            "Receiving objects: 100% (68149/68149), 51.86 MiB | 29.67 MiB/s, done.\n",
            "Resolving deltas: 100% (48151/48151), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2.10)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.5.0.dev0-cp37-none-any.whl size=2037253 sha256=8f7f03b11738195b78135cf2022503313a2cb9a9fc9502c61447cfe8466be837\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ut9ewt4a/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=ea1d4ac7a2de7bd79944b7513134938bc5dd554e15b6b06b3892dcb809c8df88\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.5.0.dev0\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hCollecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 21.8MB/s \n",
            "\u001b[?25hCollecting datasets>=1.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/43b396481a8298c6010afb93b3c1e71d4ba6f8c10797a7da8eb005e45081/datasets-1.5.0-py3-none-any.whl (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (0.3.3)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (0.70.11.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.0.0)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/af/07/bf95f398e6598202d878332280f36e589512174882536eb20d792532a57d/huggingface_hub-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.2.1->-r ./examples/question-answering/requirements.txt (line 1)) (1.15.0)\n",
            "Installing collected packages: fsspec, xxhash, huggingface-hub, datasets\n",
            "Successfully installed datasets-1.5.0 fsspec-0.8.7 huggingface-hub-0.0.7 xxhash-2.0.0\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufyiiRMKLsoW",
        "outputId": "b156c4f4-7eff-4cfa-90e7-98780638a0f4"
      },
      "source": [
        "!python ./transformers/examples/seq2seq/run_translation.py --model_name_or_path t5-small --output_dir content/tmp/zero3 \\\n",
        "--overwrite_output_dir --max_train_samples 10 --max_val_samples 100 --max_source_length 12 \\\n",
        "--max_target_length 128 --val_max_target_length 128 --do_train --num_train_epochs 1 \\\n",
        "--per_device_train_batch_size 2 --learning_rate 3e-3 --warmup_steps 8 --predict_with_generate \\\n",
        "--logging_steps 0 --save_steps 2 --eval_steps 1 --group_by_length --adafactor --dataset_name wmt16 \\\n",
        "--dataset_config ro-en --source_lang en --target_lang ro \\\n",
        "--source_prefix \"translate English to Romanian: \" --do_eval "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-30 17:36:50.764185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "03/30/2021 17:36:51 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "03/30/2021 17:36:51 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='content/tmp/zero3', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.003, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=8, logging_dir='runs/Mar30_17-36-51_74fde95eaa69', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=0, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=2, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1, dataloader_num_workers=0, past_index=-1, run_name='content/tmp/zero3', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=True, group_by_length=True, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True)\n",
            "Downloading: 2.81kB [00:00, 3.71MB/s]       \n",
            "Downloading: 3.19kB [00:00, 2.68MB/s]       \n",
            "Downloading: 41.1kB [00:00, 35.3MB/s]       \n",
            "Downloading and preparing dataset wmt16/ro-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f...\n",
            "Downloading: 100% 225M/225M [00:03<00:00, 67.5MB/s]\n",
            "Downloading: 100% 23.5M/23.5M [00:01<00:00, 12.1MB/s]\n",
            "Downloading: 100% 38.7M/38.7M [00:01<00:00, 26.8MB/s]\n",
            "Dataset wmt16 downloaded and prepared to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/t5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps8_rj02b\n",
            "Downloading: 100% 1.20k/1.20k [00:00<00:00, 1.74MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "creating metadata file for /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "https://huggingface.co/t5-small/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxrzx1vdo\n",
            "Downloading: 100% 792k/792k [00:00<00:00, 20.4MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "creating metadata file for /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "https://huggingface.co/t5-small/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpts1wkui0\n",
            "Downloading: 100% 1.39M/1.39M [00:00<00:00, 37.3MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "creating metadata file for /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "https://huggingface.co/t5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgxo4bico\n",
            "Downloading: 100% 242M/242M [00:03<00:00, 69.9MB/s]\n",
            "storing https://huggingface.co/t5-small/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "creating metadata file for /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "100% 1/1 [00:00<00:00, 328.27ba/s]\n",
            "100% 1/1 [00:00<00:00, 56.75ba/s]\n",
            "Downloading: 5.40kB [00:00, 6.43MB/s]       \n",
            "***** Running training *****\n",
            "  Num examples = 10\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5\n",
            "  0% 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:557: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n",
            " 40% 2/5 [00:00<00:01,  1.94it/s]Saving model checkpoint to content/tmp/zero3/checkpoint-2\n",
            "Configuration saved in content/tmp/zero3/checkpoint-2/config.json\n",
            "Model weights saved in content/tmp/zero3/checkpoint-2/pytorch_model.bin\n",
            "tokenizer config file saved in content/tmp/zero3/checkpoint-2/tokenizer_config.json\n",
            "Special tokens file saved in content/tmp/zero3/checkpoint-2/special_tokens_map.json\n",
            "Copy vocab file to content/tmp/zero3/checkpoint-2/spiece.model\n",
            " 80% 4/5 [00:02<00:00,  1.86it/s]Saving model checkpoint to content/tmp/zero3/checkpoint-4\n",
            "Configuration saved in content/tmp/zero3/checkpoint-4/config.json\n",
            "Model weights saved in content/tmp/zero3/checkpoint-4/pytorch_model.bin\n",
            "tokenizer config file saved in content/tmp/zero3/checkpoint-4/tokenizer_config.json\n",
            "Special tokens file saved in content/tmp/zero3/checkpoint-4/special_tokens_map.json\n",
            "Copy vocab file to content/tmp/zero3/checkpoint-4/spiece.model\n",
            "100% 5/5 [00:03<00:00,  1.54it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3.0631, 'train_samples_per_second': 1.632, 'epoch': 1.0}\n",
            "100% 5/5 [00:03<00:00,  1.64it/s]\n",
            "Saving model checkpoint to content/tmp/zero3\n",
            "Configuration saved in content/tmp/zero3/config.json\n",
            "Model weights saved in content/tmp/zero3/pytorch_model.bin\n",
            "tokenizer config file saved in content/tmp/zero3/tokenizer_config.json\n",
            "Special tokens file saved in content/tmp/zero3/special_tokens_map.json\n",
            "Copy vocab file to content/tmp/zero3/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                      =        1.0\n",
            "  init_mem_cpu_alloc_delta   =     2413MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  init_mem_gpu_alloc_delta   =      230MB\n",
            "  init_mem_gpu_peaked_delta  =        0MB\n",
            "  train_mem_cpu_alloc_delta  =       15MB\n",
            "  train_mem_cpu_peaked_delta =        0MB\n",
            "  train_mem_gpu_alloc_delta  =      231MB\n",
            "  train_mem_gpu_peaked_delta =      194MB\n",
            "  train_runtime              = 0:00:03.06\n",
            "  train_samples              =         10\n",
            "  train_samples_per_second   =      1.632\n",
            "03/30/2021 17:37:44 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "100% 13/13 [00:11<00:00,  1.18it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        1.0\n",
            "  eval_bleu                 =      2.259\n",
            "  eval_gen_len              =      15.61\n",
            "  eval_loss                 =     3.7628\n",
            "  eval_mem_cpu_alloc_delta  =        1MB\n",
            "  eval_mem_cpu_peaked_delta =        0MB\n",
            "  eval_mem_gpu_alloc_delta  =        0MB\n",
            "  eval_mem_gpu_peaked_delta =      171MB\n",
            "  eval_runtime              = 0:00:11.58\n",
            "  eval_samples              =        100\n",
            "  eval_samples_per_second   =      8.632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI0W_ZRoL78h",
        "outputId": "5631391b-eb94-4e79-d05e-4a7d72c14078"
      },
      "source": [
        "!python ./transformers/examples/seq2seq/run_translation.py --model_name_or_path t5-small --output_dir /tmp/zero3 \\\n",
        "--overwrite_output_dir --max_train_samples 10 --max_val_samples 100 --max_source_length 12 \\\n",
        "--max_target_length 128 --val_max_target_length 128 --do_train --num_train_epochs 1 \\\n",
        "--per_device_train_batch_size 2 --learning_rate 3e-3 --warmup_steps 8 --predict_with_generate \\\n",
        "--logging_steps 0 --save_steps 2 --eval_steps 1 --group_by_length --adafactor --dataset_name wmt16 \\\n",
        "--dataset_config ro-en --source_lang en --target_lang ro \\\n",
        "--source_prefix \"translate English to Romanian: \" --do_eval  \\\n",
        "--fp16_full_eval"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-30 17:38:45.642559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "03/30/2021 17:38:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "03/30/2021 17:38:46 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/tmp/zero3', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.003, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=8, logging_dir='runs/Mar30_17-38-46_74fde95eaa69', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=0, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=2, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=True, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1, dataloader_num_workers=0, past_index=-1, run_name='/tmp/zero3', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=True, group_by_length=True, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True)\n",
            "03/30/2021 17:38:47 - WARNING - datasets.builder -   Reusing dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f)\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "03/30/2021 17:38:49 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-ef72cbf5b91b6f21.arrow\n",
            "03/30/2021 17:38:49 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-36025bb4a15c4f75.arrow\n",
            "***** Running training *****\n",
            "  Num examples = 10\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5\n",
            "  0% 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:557: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n",
            " 40% 2/5 [00:00<00:00,  3.54it/s]Saving model checkpoint to /tmp/zero3/checkpoint-2\n",
            "Configuration saved in /tmp/zero3/checkpoint-2/config.json\n",
            "Model weights saved in /tmp/zero3/checkpoint-2/pytorch_model.bin\n",
            "tokenizer config file saved in /tmp/zero3/checkpoint-2/tokenizer_config.json\n",
            "Special tokens file saved in /tmp/zero3/checkpoint-2/special_tokens_map.json\n",
            "Copy vocab file to /tmp/zero3/checkpoint-2/spiece.model\n",
            " 80% 4/5 [00:01<00:00,  2.44it/s]Saving model checkpoint to /tmp/zero3/checkpoint-4\n",
            "Configuration saved in /tmp/zero3/checkpoint-4/config.json\n",
            "Model weights saved in /tmp/zero3/checkpoint-4/pytorch_model.bin\n",
            "tokenizer config file saved in /tmp/zero3/checkpoint-4/tokenizer_config.json\n",
            "Special tokens file saved in /tmp/zero3/checkpoint-4/special_tokens_map.json\n",
            "Copy vocab file to /tmp/zero3/checkpoint-4/spiece.model\n",
            "100% 5/5 [00:02<00:00,  1.79it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2.6562, 'train_samples_per_second': 1.882, 'epoch': 1.0}\n",
            "100% 5/5 [00:02<00:00,  1.89it/s]\n",
            "Saving model checkpoint to /tmp/zero3\n",
            "Configuration saved in /tmp/zero3/config.json\n",
            "Model weights saved in /tmp/zero3/pytorch_model.bin\n",
            "tokenizer config file saved in /tmp/zero3/tokenizer_config.json\n",
            "Special tokens file saved in /tmp/zero3/special_tokens_map.json\n",
            "Copy vocab file to /tmp/zero3/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                      =        1.0\n",
            "  init_mem_cpu_alloc_delta   =     2439MB\n",
            "  init_mem_cpu_peaked_delta  =        0MB\n",
            "  init_mem_gpu_alloc_delta   =      230MB\n",
            "  init_mem_gpu_peaked_delta  =        0MB\n",
            "  train_mem_cpu_alloc_delta  =       16MB\n",
            "  train_mem_cpu_peaked_delta =        7MB\n",
            "  train_mem_gpu_alloc_delta  =      231MB\n",
            "  train_mem_gpu_peaked_delta =      194MB\n",
            "  train_runtime              = 0:00:02.65\n",
            "  train_samples              =         10\n",
            "  train_samples_per_second   =      1.882\n",
            "03/30/2021 17:38:57 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 8\n",
            "100% 13/13 [00:14<00:00,  1.08s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        1.0\n",
            "  eval_bleu                 =     2.2417\n",
            "  eval_gen_len              =       15.5\n",
            "  eval_loss                 =     3.7617\n",
            "  eval_mem_cpu_alloc_delta  =        2MB\n",
            "  eval_mem_cpu_peaked_delta =        0MB\n",
            "  eval_mem_gpu_alloc_delta  =     -231MB\n",
            "  eval_mem_gpu_peaked_delta =      262MB\n",
            "  eval_runtime              = 0:00:14.87\n",
            "  eval_samples              =        100\n",
            "  eval_samples_per_second   =      6.724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXkAxwZMNL2e",
        "outputId": "4763243c-d13c-4072-a260-188991290d2a"
      },
      "source": [
        "# how much more time it takes during evulation while using fp16?\n",
        "(14.87/11.58 - 1)*100"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.41105354058722"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I35qhGlbOZJ2"
      },
      "source": [
        "## Conclusion:\n",
        "It takes around 28% more time when we use FP16 flag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MyJdl6BOHnf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}