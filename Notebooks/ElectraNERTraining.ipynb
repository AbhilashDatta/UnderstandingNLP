{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ElectraNERTraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkgN90WxD1+R1i4BYwGIRV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "abc4cd7125024b018d3f570fc505b83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5a9abcfcba54c728cfc4c4c66cffa57",
              "IPY_MODEL_ab80cb0f812b4fb391f6114f9d5f0f84",
              "IPY_MODEL_75525b6d15a24997892d086b77743003"
            ],
            "layout": "IPY_MODEL_5131d01ab9e3400f8b89f148faee7fb3"
          }
        },
        "e5a9abcfcba54c728cfc4c4c66cffa57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5c5f48e969f45abb44995b7e561d701",
            "placeholder": "​",
            "style": "IPY_MODEL_4880b24376d54e49ac9f872d587d8c23",
            "value": "Upload file pytorch_model.bin: 100%"
          }
        },
        "ab80cb0f812b4fb391f6114f9d5f0f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22d27973315b4f08a3cea6e1168ae129",
            "max": 435662513,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5292363d81a49a992cd3503a26a87c1",
            "value": 435662513
          }
        },
        "75525b6d15a24997892d086b77743003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ef55e3265ba439ca3596cfc93929485",
            "placeholder": "​",
            "style": "IPY_MODEL_51ab8187c864407eb7b338810d687c9d",
            "value": " 415M/415M [05:06&lt;00:00, 970kB/s]"
          }
        },
        "5131d01ab9e3400f8b89f148faee7fb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c5f48e969f45abb44995b7e561d701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4880b24376d54e49ac9f872d587d8c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22d27973315b4f08a3cea6e1168ae129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5292363d81a49a992cd3503a26a87c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ef55e3265ba439ca3596cfc93929485": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51ab8187c864407eb7b338810d687c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/UnderstandingNLP/blob/master/Notebooks/ElectraNERTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo6506iK7Aue",
        "outputId": "c25d1972-52ad-466f-bf9b-fd69e733957d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 108324, done.\u001b[K\n",
            "remote: Total 108324 (delta 0), reused 0 (delta 0), pack-reused 108324\u001b[K\n",
            "Receiving objects: 100% (108324/108324), 94.59 MiB | 20.28 MiB/s, done.\n",
            "Resolving deltas: 100% (78914/78914), done.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3958150 sha256=d44cfb661457f6b2e4ba1caebf3b4500d68cdb8d6aae3240df4b18c2ce89b209\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rjvbe4gy/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "# !git clone -b val-to-eval https://github.com/bhadreshpsavani/transformers.git\n",
        "%cd transformers\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r ./examples/pytorch/token-classification/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WApvqDY_709t",
        "outputId": "eea3a701-d712-4cd9-c6cb-fadd0b376a6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20 kB 38.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 30 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 40 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 51 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 61 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 65 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 35.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 64.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 73.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 75.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 65.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 75.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 64.1 MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ0Am-Y88OLU",
        "outputId": "f81a8644-996c-4364-def3-0f526ccda0cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python transformers/examples/pytorch/token-classification/run_ner.py \\\n",
        "--model_name_or_path google/electra-base-discriminator \\\n",
        "--dataset_name conll2003 \\\n",
        "--output_dir /tmp/test-ner \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCd2PkkY8DwC",
        "outputId": "a9c7e772-6c80-45a9-94e4-cf3af93b73c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04/02/2022 11:05:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/02/2022 11:05:32 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/test-ner/runs/Apr02_11-05-32_02ae93040f0f,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=/tmp/test-ner,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/test-ner,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/02/2022 11:05:33 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/conll2003/conll2003.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpum5jgykz\n",
            "Downloading builder script: 9.52kB [00:00, 11.9MB/s]       \n",
            "04/02/2022 11:05:33 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/conll2003/conll2003.py in cache at /root/.cache/huggingface/datasets/downloads/a8c91c5a1c09f00da088576f4b36d7fdbfa2131c3391eed12f0733a9cbf134f7.9a287555d9dac893b8fcfe877c947c05fa9523e0f8cd375879ca2ea11b0ae8c0.py\n",
            "04/02/2022 11:05:33 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a8c91c5a1c09f00da088576f4b36d7fdbfa2131c3391eed12f0733a9cbf134f7.9a287555d9dac893b8fcfe877c947c05fa9523e0f8cd375879ca2ea11b0ae8c0.py\n",
            "04/02/2022 11:05:33 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/conll2003/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpmbdb15t5\n",
            "Downloading metadata: 3.79kB [00:00, 6.06MB/s]       \n",
            "04/02/2022 11:05:33 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/conll2003/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/88148114a23edb05594b0e825a6f803d19cbbaf728aef548b479c9621cdd3b9b.cd511f49e0ffec7f38e01dcd27d113f652ffd4f4a91a247aa8b344bdf1813174\n",
            "04/02/2022 11:05:33 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/88148114a23edb05594b0e825a6f803d19cbbaf728aef548b479c9621cdd3b9b.cd511f49e0ffec7f38e01dcd27d113f652ffd4f4a91a247aa8b344bdf1813174\n",
            "04/02/2022 11:05:33 - INFO - datasets.builder - No config specified, defaulting to first: conll2003/conll2003\n",
            "04/02/2022 11:05:33 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/conll2003/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee\n",
            "04/02/2022 11:05:33 - INFO - datasets.builder - Generating dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
            "Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n",
            "04/02/2022 11:05:33 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "04/02/2022 11:05:34 - INFO - datasets.utils.file_utils - https://data.deepai.org/conll2003.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpsg9u0ms1\n",
            "Downloading data: 100% 983k/983k [00:00<00:00, 14.8MB/s]\n",
            "04/02/2022 11:05:34 - INFO - datasets.utils.file_utils - storing https://data.deepai.org/conll2003.zip in cache at /root/.cache/huggingface/datasets/downloads/2993ad73bf341ce483f06d7cab237d62db303b00ad5086eef4e64f52b84123f6\n",
            "04/02/2022 11:05:34 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/2993ad73bf341ce483f06d7cab237d62db303b00ad5086eef4e64f52b84123f6\n",
            "04/02/2022 11:05:34 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "04/02/2022 11:05:34 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "04/02/2022 11:05:34 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "04/02/2022 11:05:34 - INFO - datasets.builder - Generating train split\n",
            "04/02/2022 11:05:37 - INFO - datasets.builder - Generating validation split\n",
            "04/02/2022 11:05:37 - INFO - datasets.builder - Generating test split\n",
            "04/02/2022 11:05:38 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 372.43it/s]\n",
            "[INFO|hub.py:583] 2022-04-02 11:05:39,054 >> https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_sc9cwt1\n",
            "Downloading: 100% 666/666 [00:00<00:00, 664kB/s]\n",
            "[INFO|hub.py:587] 2022-04-02 11:05:39,415 >> storing https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
            "[INFO|hub.py:595] 2022-04-02 11:05:39,415 >> creating metadata file for /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
            "[INFO|configuration_utils.py:653] 2022-04-02 11:05:39,415 >> loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
            "[INFO|configuration_utils.py:689] 2022-04-02 11:05:39,417 >> Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.18.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-02 11:05:39,775 >> https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpm_z3xhxh\n",
            "Downloading: 100% 27.0/27.0 [00:00<00:00, 23.1kB/s]\n",
            "[INFO|hub.py:587] 2022-04-02 11:05:40,137 >> storing https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/6f8b3f5095b6f44f5c75cee3c56b971b3208b08132ba2f9fb775a4a7b7140942.4f2213f5603276adf12967b32e4444c0f187f34ca4f8b22a65f03e13514589e9\n",
            "[INFO|hub.py:595] 2022-04-02 11:05:40,137 >> creating metadata file for /root/.cache/huggingface/transformers/6f8b3f5095b6f44f5c75cee3c56b971b3208b08132ba2f9fb775a4a7b7140942.4f2213f5603276adf12967b32e4444c0f187f34ca4f8b22a65f03e13514589e9\n",
            "[INFO|configuration_utils.py:653] 2022-04-02 11:05:40,495 >> loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
            "[INFO|configuration_utils.py:689] 2022-04-02 11:05:40,495 >> Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.18.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-02 11:05:41,212 >> https://huggingface.co/google/electra-base-discriminator/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgapfwk4b\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 684kB/s]\n",
            "[INFO|hub.py:587] 2022-04-02 11:05:41,914 >> storing https://huggingface.co/google/electra-base-discriminator/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:595] 2022-04-02 11:05:41,914 >> creating metadata file for /root/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|hub.py:583] 2022-04-02 11:05:42,273 >> https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzat5nbyc\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 1.10MB/s]\n",
            "[INFO|hub.py:587] 2022-04-02 11:05:43,059 >> storing https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|hub.py:595] 2022-04-02 11:05:43,059 >> creating metadata file for /root/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-02 11:05:44,143 >> loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/fe616facc71d8e3afc69de3edac76bf1e4a0a741e80d9a99a2cc6a9a8f5f74b5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-02 11:05:44,143 >> loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/81840ac426bf0d690bfb69a4ec7d706e8853d8ab309e7decb6b72ab939d6682e.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-02 11:05:44,143 >> loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-02 11:05:44,143 >> loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-02 11:05:44,143 >> loading file https://huggingface.co/google/electra-base-discriminator/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/6f8b3f5095b6f44f5c75cee3c56b971b3208b08132ba2f9fb775a4a7b7140942.4f2213f5603276adf12967b32e4444c0f187f34ca4f8b22a65f03e13514589e9\n",
            "[INFO|configuration_utils.py:653] 2022-04-02 11:05:44,501 >> loading configuration file https://huggingface.co/google/electra-base-discriminator/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/7d1569a4df2372d67341bda716bce4e3edf3e3ffadb97251bc4b6b35d459f624.57c13443a51769ce892714c93bb3ee3952bad66d7d9662d9de382b808377c3f8\n",
            "[INFO|configuration_utils.py:689] 2022-04-02 11:05:44,502 >> Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.18.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-02 11:05:44,904 >> https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8diu6j9w\n",
            "Downloading: 100% 420M/420M [00:06<00:00, 65.5MB/s]\n",
            "[INFO|hub.py:587] 2022-04-02 11:05:51,726 >> storing https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1\n",
            "[INFO|hub.py:595] 2022-04-02 11:05:51,726 >> creating metadata file for /root/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1\n",
            "[INFO|modeling_utils.py:1771] 2022-04-02 11:05:51,727 >> loading weights file https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/aed576b8aec823c870feda40d60bd803ac8e40056ecb7d7f43dd0b2bfd82e373.db390a2059e53ead2bb00e1a2f8cd50b0a47e1969d180cd70339ec3f6f29dce1\n",
            "[WARNING|modeling_utils.py:2049] 2022-04-02 11:05:53,216 >> Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2060] 2022-04-02 11:05:53,216 >> Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/15 [00:00<?, ?ba/s]04/02/2022 11:05:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee/cache-d2dacb786388de53.arrow\n",
            "Running tokenizer on train dataset: 100% 15/15 [00:02<00:00,  7.03ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/4 [00:00<?, ?ba/s]04/02/2022 11:05:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee/cache-bda324f9e1aecc80.arrow\n",
            "Running tokenizer on validation dataset: 100% 4/4 [00:00<00:00,  9.02ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/4 [00:00<?, ?ba/s]04/02/2022 11:05:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee/cache-5476e183fcd6ce7a.arrow\n",
            "Running tokenizer on prediction dataset: 100% 4/4 [00:00<00:00,  9.53ba/s]\n",
            "04/02/2022 11:05:56 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/seqeval/seqeval.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpj01tmvno\n",
            "Downloading builder script: 6.33kB [00:00, 7.13MB/s]       \n",
            "04/02/2022 11:05:56 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/seqeval/seqeval.py in cache at /root/.cache/huggingface/datasets/downloads/97fd0798f93700f6de84066108b8c335661ac2940e504ee1492a9e287a12e3e2.63490f48f0656d09ada2c86daed79587694915df82dd311e407b049ad2f525eb.py\n",
            "04/02/2022 11:05:56 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/97fd0798f93700f6de84066108b8c335661ac2940e504ee1492a9e287a12e3e2.63490f48f0656d09ada2c86daed79587694915df82dd311e407b049ad2f525eb.py\n",
            "[INFO|trainer.py:567] 2022-04-02 11:06:07,484 >> The following columns in the training set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id, pos_tags, ner_tags, chunk_tags. If tokens, id, pos_tags, ner_tags, chunk_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1290] 2022-04-02 11:06:07,498 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2022-04-02 11:06:07,498 >>   Num examples = 14042\n",
            "[INFO|trainer.py:1292] 2022-04-02 11:06:07,498 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1293] 2022-04-02 11:06:07,498 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1294] 2022-04-02 11:06:07,498 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1295] 2022-04-02 11:06:07,498 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2022-04-02 11:06:07,498 >>   Total optimization steps = 5268\n",
            "{'loss': 0.1864, 'learning_rate': 4.525436598329537e-05, 'epoch': 0.28}\n",
            "  9% 500/5268 [00:50<09:50,  8.07it/s][INFO|trainer.py:2166] 2022-04-02 11:06:58,129 >> Saving model checkpoint to /tmp/test-ner/checkpoint-500\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:06:58,129 >> Configuration saved in /tmp/test-ner/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:06:59,161 >> Model weights saved in /tmp/test-ner/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:06:59,162 >> tokenizer config file saved in /tmp/test-ner/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:06:59,162 >> Special tokens file saved in /tmp/test-ner/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.0791, 'learning_rate': 4.050873196659074e-05, 'epoch': 0.57}\n",
            " 19% 1000/5268 [01:47<07:51,  9.06it/s][INFO|trainer.py:2166] 2022-04-02 11:07:54,563 >> Saving model checkpoint to /tmp/test-ner/checkpoint-1000\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:07:54,564 >> Configuration saved in /tmp/test-ner/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:07:55,716 >> Model weights saved in /tmp/test-ner/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:07:55,717 >> tokenizer config file saved in /tmp/test-ner/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:07:55,717 >> Special tokens file saved in /tmp/test-ner/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.0661, 'learning_rate': 3.5763097949886106e-05, 'epoch': 0.85}\n",
            " 28% 1500/5268 [02:43<06:14, 10.06it/s][INFO|trainer.py:2166] 2022-04-02 11:08:50,777 >> Saving model checkpoint to /tmp/test-ner/checkpoint-1500\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:08:50,778 >> Configuration saved in /tmp/test-ner/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:08:51,966 >> Model weights saved in /tmp/test-ner/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:08:51,966 >> tokenizer config file saved in /tmp/test-ner/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:08:51,966 >> Special tokens file saved in /tmp/test-ner/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 0.0461, 'learning_rate': 3.1017463933181475e-05, 'epoch': 1.14}\n",
            " 38% 2000/5268 [03:40<05:43,  9.51it/s][INFO|trainer.py:2166] 2022-04-02 11:09:47,565 >> Saving model checkpoint to /tmp/test-ner/checkpoint-2000\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:09:47,566 >> Configuration saved in /tmp/test-ner/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:09:48,591 >> Model weights saved in /tmp/test-ner/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:09:48,592 >> tokenizer config file saved in /tmp/test-ner/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:09:48,592 >> Special tokens file saved in /tmp/test-ner/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 0.0372, 'learning_rate': 2.6271829916476843e-05, 'epoch': 1.42}\n",
            " 47% 2500/5268 [04:37<04:46,  9.66it/s][INFO|trainer.py:2166] 2022-04-02 11:10:44,557 >> Saving model checkpoint to /tmp/test-ner/checkpoint-2500\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:10:44,558 >> Configuration saved in /tmp/test-ner/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:10:45,641 >> Model weights saved in /tmp/test-ner/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:10:45,642 >> tokenizer config file saved in /tmp/test-ner/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:10:45,642 >> Special tokens file saved in /tmp/test-ner/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.0316, 'learning_rate': 2.152619589977221e-05, 'epoch': 1.71}\n",
            " 57% 3000/5268 [05:33<03:43, 10.15it/s][INFO|trainer.py:2166] 2022-04-02 11:11:41,180 >> Saving model checkpoint to /tmp/test-ner/checkpoint-3000\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:11:41,181 >> Configuration saved in /tmp/test-ner/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:11:42,434 >> Model weights saved in /tmp/test-ner/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:11:42,434 >> tokenizer config file saved in /tmp/test-ner/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:11:42,434 >> Special tokens file saved in /tmp/test-ner/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.0272, 'learning_rate': 1.678056188306758e-05, 'epoch': 1.99}\n",
            " 66% 3500/5268 [06:30<03:19,  8.87it/s][INFO|trainer.py:2166] 2022-04-02 11:12:37,937 >> Saving model checkpoint to /tmp/test-ner/checkpoint-3500\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:12:37,938 >> Configuration saved in /tmp/test-ner/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:12:39,153 >> Model weights saved in /tmp/test-ner/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:12:39,153 >> tokenizer config file saved in /tmp/test-ner/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:12:39,154 >> Special tokens file saved in /tmp/test-ner/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.0172, 'learning_rate': 1.2034927866362947e-05, 'epoch': 2.28}\n",
            " 76% 4000/5268 [07:27<02:06, 10.00it/s][INFO|trainer.py:2166] 2022-04-02 11:13:34,799 >> Saving model checkpoint to /tmp/test-ner/checkpoint-4000\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:13:34,801 >> Configuration saved in /tmp/test-ner/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:13:35,978 >> Model weights saved in /tmp/test-ner/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:13:35,979 >> tokenizer config file saved in /tmp/test-ner/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:13:35,979 >> Special tokens file saved in /tmp/test-ner/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.0155, 'learning_rate': 7.289293849658315e-06, 'epoch': 2.56}\n",
            " 85% 4500/5268 [08:23<01:18,  9.82it/s][INFO|trainer.py:2166] 2022-04-02 11:14:31,414 >> Saving model checkpoint to /tmp/test-ner/checkpoint-4500\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:14:31,415 >> Configuration saved in /tmp/test-ner/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:14:32,787 >> Model weights saved in /tmp/test-ner/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:14:32,788 >> tokenizer config file saved in /tmp/test-ner/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:14:32,788 >> Special tokens file saved in /tmp/test-ner/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 0.0145, 'learning_rate': 2.5436598329536827e-06, 'epoch': 2.85}\n",
            " 95% 5000/5268 [09:21<00:27,  9.60it/s][INFO|trainer.py:2166] 2022-04-02 11:15:28,699 >> Saving model checkpoint to /tmp/test-ner/checkpoint-5000\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:15:28,700 >> Configuration saved in /tmp/test-ner/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:15:29,849 >> Model weights saved in /tmp/test-ner/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:15:29,850 >> tokenizer config file saved in /tmp/test-ner/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:15:29,850 >> Special tokens file saved in /tmp/test-ner/checkpoint-5000/special_tokens_map.json\n",
            "100% 5268/5268 [09:53<00:00, 10.15it/s][INFO|trainer.py:1530] 2022-04-02 11:16:01,111 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 593.6168, 'train_samples_per_second': 70.965, 'train_steps_per_second': 8.874, 'train_loss': 0.050021743222267774, 'epoch': 3.0}\n",
            "100% 5268/5268 [09:53<00:00,  8.87it/s]\n",
            "[INFO|trainer.py:2166] 2022-04-02 11:16:01,122 >> Saving model checkpoint to /tmp/test-ner\n",
            "[INFO|configuration_utils.py:440] 2022-04-02 11:16:01,123 >> Configuration saved in /tmp/test-ner/config.json\n",
            "[INFO|modeling_utils.py:1377] 2022-04-02 11:16:02,272 >> Model weights saved in /tmp/test-ner/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-02 11:16:02,272 >> tokenizer config file saved in /tmp/test-ner/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-02 11:16:02,273 >> Special tokens file saved in /tmp/test-ner/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =       0.05\n",
            "  train_runtime            = 0:09:53.61\n",
            "  train_samples            =      14042\n",
            "  train_samples_per_second =     70.965\n",
            "  train_steps_per_second   =      8.874\n",
            "04/02/2022 11:16:02 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:567] 2022-04-02 11:16:02,429 >> The following columns in the evaluation set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id, pos_tags, ner_tags, chunk_tags. If tokens, id, pos_tags, ner_tags, chunk_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-04-02 11:16:02,486 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-04-02 11:16:02,486 >>   Num examples = 3251\n",
            "[INFO|trainer.py:2421] 2022-04-02 11:16:02,486 >>   Batch size = 8\n",
            "100% 406/407 [00:09<00:00, 36.50it/s]04/02/2022 11:16:13 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "100% 407/407 [00:11<00:00, 36.72it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.9898\n",
            "  eval_f1                 =     0.9501\n",
            "  eval_loss               =     0.0461\n",
            "  eval_precision          =     0.9471\n",
            "  eval_recall             =      0.953\n",
            "  eval_runtime            = 0:00:11.10\n",
            "  eval_samples            =       3251\n",
            "  eval_samples_per_second =    292.675\n",
            "  eval_steps_per_second   =     36.641\n",
            "04/02/2022 11:16:13 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:567] 2022-04-02 11:16:13,605 >> The following columns in the test set  don't have a corresponding argument in `ElectraForTokenClassification.forward` and have been ignored: tokens, id, pos_tags, ner_tags, chunk_tags. If tokens, id, pos_tags, ner_tags, chunk_tags are not expected by `ElectraForTokenClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-04-02 11:16:13,607 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2418] 2022-04-02 11:16:13,607 >>   Num examples = 3454\n",
            "[INFO|trainer.py:2421] 2022-04-02 11:16:13,608 >>   Batch size = 8\n",
            "100% 430/432 [00:08<00:00, 56.69it/s]04/02/2022 11:16:23 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "***** predict metrics *****\n",
            "  predict_accuracy           =     0.9813\n",
            "  predict_f1                 =     0.9137\n",
            "  predict_loss               =     0.1251\n",
            "  predict_precision          =     0.9098\n",
            "  predict_recall             =     0.9177\n",
            "  predict_runtime            = 0:00:10.11\n",
            "  predict_samples_per_second =    341.368\n",
            "  predict_steps_per_second   =     42.696\n",
            "100% 432/432 [00:10<00:00, 40.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/tmp/test-ner\""
      ],
      "metadata": {
        "id": "ubgZRsS8_rnF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, ElectraForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = ElectraForTokenClassification.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "4V8PzvBr8MvV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "inputs = tokenizer(\"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  logits = model(**inputs).logits\n",
        "\n",
        "predicted_token_class_ids = logits.argmax(-1)\n",
        "\n",
        "# Note that tokens are classified rather then input words which means that\n",
        "# there might be more predicted token classes than words.\n",
        "# Multiple token classes might account for the same word\n",
        "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
        "predicted_tokens_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moPN1x9M_zn7",
        "outputId": "6ceed429-2fd8-459b-ab81-076aa403ac17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-LOC', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'I-LOC']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txc_HC3t_3yH",
        "outputId": "87ea6c1f-577e-470a-d6a7-8676b595bc99"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[31mWARNING! `transformers-cli login` is deprecated and will be removed in v5. Please use `huggingface-cli login` instead.\u001b[0m\n",
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        \n",
            "Username: bhadresh-savani\n",
            "Password: \n",
            "ERROR:root:HfApi.login: This method is deprecated in favor of `set_access_token`.\n",
            "Login successful\n",
            "Your token: USSrKekMAIdNYfcxwutSzdOSvEXmxQJcQWvJIQYlcxLUEOdLuBlwYFYRPUksnUhbBXDghVggoqQhdHoXexdlwHWVyytklJLRARIxPrkYfecZgatctBQwalZlMrMVWVyG \n",
            "\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install git-lfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQCV72tiAJiO",
        "outputId": "e092f5ac-33fd-4597-8ac1-57c357e41bcb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (2,651 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"bhadreshpsavani@gmail.com\"\n",
        "!git config --global user.name \"bhadreshpsavani\"\n",
        "!git config --global user.password \"*******\""
      ],
      "metadata": {
        "id": "g25HwdSeAWku"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub('bhadresh-savani/electra-base-discriminator-finetuned-conll03-english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "abc4cd7125024b018d3f570fc505b83b",
            "e5a9abcfcba54c728cfc4c4c66cffa57",
            "ab80cb0f812b4fb391f6114f9d5f0f84",
            "75525b6d15a24997892d086b77743003",
            "5131d01ab9e3400f8b89f148faee7fb3",
            "d5c5f48e969f45abb44995b7e561d701",
            "4880b24376d54e49ac9f872d587d8c23",
            "22d27973315b4f08a3cea6e1168ae129",
            "e5292363d81a49a992cd3503a26a87c1",
            "2ef55e3265ba439ca3596cfc93929485",
            "51ab8187c864407eb7b338810d687c9d"
          ]
        },
        "id": "CmTLHMtpAekR",
        "outputId": "61afb55d-0c1d-4e0a-a1fa-4d53d8e24050"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py:1004: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
            "  FutureWarning,\n",
            "Cloning https://huggingface.co/bhadresh-savani/electra-base-discriminator-finetuned-conll03-english into local empty directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload file pytorch_model.bin:   0%|          | 3.34k/415M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abc4cd7125024b018d3f570fc505b83b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "To https://huggingface.co/bhadresh-savani/electra-base-discriminator-finetuned-conll03-english\n",
            "   2dca4f4..12fde28  main -> main\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/bhadresh-savani/electra-base-discriminator-finetuned-conll03-english/commit/12fde28ae122414ec7535a214980e8587266e4e0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub('bhadresh-savani/electra-base-discriminator-finetuned-conll03-english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "nw-mlUKtAztv",
        "outputId": "18e19171-79d5-400c-80cb-52feefd9ed5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "To https://huggingface.co/bhadresh-savani/electra-base-discriminator-finetuned-conll03-english\n",
            "   12fde28..2055377  main -> main\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/bhadresh-savani/electra-base-discriminator-finetuned-conll03-english/commit/205537713fb3a43a285f889b7f03c7e755eec209'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-aLsI7VsBQ5r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}