{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parapharse_Question_Generation_with_T5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPM4THh6UdYpMSf5qR2yT+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a63e119f1e314d48b58c348c72c2fbb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_943e1b2a905d47b78a3d33683ebd6a0c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f624615879734b38b11782dc4ce50ca4",
              "IPY_MODEL_9f123cce69d24b6da7a11260ca09ddcc"
            ]
          }
        },
        "943e1b2a905d47b78a3d33683ebd6a0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f624615879734b38b11782dc4ce50ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_129f52f987c14343a704979c3ad9fd75",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 891691413,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 891691413,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eec06deae4014eff834f1ba4be6822c3"
          }
        },
        "9f123cce69d24b6da7a11260ca09ddcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_38ba3e8ebc66402bae1e25f54f1411f8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 892M/892M [00:23&lt;00:00, 38.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58890b7c5ec0457590b4d4a97b866d45"
          }
        },
        "129f52f987c14343a704979c3ad9fd75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eec06deae4014eff834f1ba4be6822c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38ba3e8ebc66402bae1e25f54f1411f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58890b7c5ec0457590b4d4a97b866d45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/UnderstandingNLP/blob/master/Parapharse_Question_Generation_with_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SnbXc3qpm3o",
        "outputId": "3c510b4a-ac31-44ff-bff3-4beea30ba26f"
      },
      "source": [
        "!pip install -q transformers\r\n",
        "!pip install -q sentencepiece"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 26.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 26.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 19.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 15.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 16.1MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 13.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 14.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 15.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 14.1MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 14.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 14.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 14.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 14.1MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 14.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 14.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 14.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8jThpFYp4Ee"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a63e119f1e314d48b58c348c72c2fbb7",
            "943e1b2a905d47b78a3d33683ebd6a0c",
            "f624615879734b38b11782dc4ce50ca4",
            "9f123cce69d24b6da7a11260ca09ddcc",
            "129f52f987c14343a704979c3ad9fd75",
            "eec06deae4014eff834f1ba4be6822c3",
            "38ba3e8ebc66402bae1e25f54f1411f8",
            "58890b7c5ec0457590b4d4a97b866d45"
          ]
        },
        "id": "f7A85ConptRW",
        "outputId": "9345f3f5-1c77-4a9f-ab54-2f2ce137ee0c"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ramsrigouthamg/t5_paraphraser\")\r\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"ramsrigouthamg/t5_paraphraser\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a63e119f1e314d48b58c348c72c2fbb7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891691413.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFN1WX6Xp5bv",
        "outputId": "a3ca7545-51f5-492f-96f3-5539ed53ed57"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print (\"device \",device)\r\n",
        "model = model.to(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device  cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u0Zd3wTqrzP"
      },
      "source": [
        "def infernce(sentence):\r\n",
        "\r\n",
        "  text =  \"paraphrase: \" + sentence + \" </s>\"\r\n",
        "  max_len = 256\r\n",
        "\r\n",
        "  encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\r\n",
        "  input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\r\n",
        "\r\n",
        "\r\n",
        "  # set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\r\n",
        "  beam_outputs = model.generate(\r\n",
        "      input_ids=input_ids, attention_mask=attention_masks,\r\n",
        "      do_sample=True,\r\n",
        "      max_length=256,\r\n",
        "      top_k=120,\r\n",
        "      top_p=0.98,\r\n",
        "      early_stopping=True,\r\n",
        "      num_return_sequences=10\r\n",
        "  )\r\n",
        "\r\n",
        "  print (\"\\nOriginal Question ::\")\r\n",
        "  print (sentence)\r\n",
        "  print (\"\\n\")\r\n",
        "  print (\"Paraphrased Questions :: \")\r\n",
        "  final_outputs =[]\r\n",
        "  for beam_output in beam_outputs:\r\n",
        "      sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\r\n",
        "      if sent.lower() != sentence.lower() and sent not in final_outputs:\r\n",
        "          final_outputs.append(sent)\r\n",
        "\r\n",
        "  for i, final_output in enumerate(final_outputs):\r\n",
        "      print(\"{}: {}\".format(i, final_output))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TLJ6H4Pp9DP",
        "outputId": "4d1c6bd5-9314-48d0-e253-e6308401c97c"
      },
      "source": [
        "sentence = \"Which course should I take to get started in data science?\"\r\n",
        "infernce(sentence)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Original Question ::\n",
            "Which course should I take to get started in data science?\n",
            "\n",
            "\n",
            "Paraphrased Questions :: \n",
            "0: How do you learn data science from basics?\n",
            "1: How can I learn Data Science independently from home?\n",
            "2: Where and when should one start with Data Science?\n",
            "3: What is the best course to study data science?\n",
            "4: What is the best course for a data scientist?\n",
            "5: What are the best resources for learning data science?\n",
            "6: What are the course requirements for Data Science?\n",
            "7: How can I get started with data science?\n",
            "8: Which is best course to learn & improve data science?\n",
            "9: What are the good courses to start data science in a college?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bke0Ub7PqEHV",
        "outputId": "d78ef969-47d1-4da6-fc14-adf31f30006a"
      },
      "source": [
        "sentence = \"What are the ingredients required to bake a perfect cake?\"\r\n",
        "infernce(sentence)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Original Question ::\n",
            "What are the ingredients required to bake a perfect cake?\n",
            "\n",
            "\n",
            "Paraphrased Questions :: \n",
            "0: What is the recipe for a great recipe for a classic cake?\n",
            "1: What are some ingredients required to bake a cake?\n",
            "2: What would a typical 24 pound cake look like?\n",
            "3: What are all the ingredients used in a delicious cake?\n",
            "4: What is the basic recipe to bake a perfect cake?\n",
            "5: What is necessary to bake a perfect cake?\n",
            "6: What do you need to bake a cake?\n",
            "7: What ingredients should I use to make a good cake?\n",
            "8: What ingredients are needed to bake a schmancy cake?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMItiBs2rHMH",
        "outputId": "b44ecaec-2822-4b77-ee47-876cec324532"
      },
      "source": [
        "sentence = \"What is the best possible approach to learn aeronautical engineering?\"\r\n",
        "infernce(sentence)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Original Question ::\n",
            "What is the best possible approach to learn aeronautical engineering?\n",
            "\n",
            "\n",
            "Paraphrased Questions :: \n",
            "0: What is the best way for studying aeronautical engineering?\n",
            "1: What are the best ways to learn aeronautical engineering?\n",
            "2: Which are the best courses to study aeronautical engineering?\n",
            "3: What are the best places to learn aeronautical engineering?\n",
            "4: What is the best way to learn aeronautical engineering?\n",
            "5: How should I get involved in aeronautical engineering?\n",
            "6: What is the best way to study aeronautical engineering?\n",
            "7: Which is the best time to learn aeronautical engineering?\n",
            "8: What is the best route to master aeronautical engineering?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oOjk0IQrI_-",
        "outputId": "e1fe5900-3794-48cc-fa5c-edc980b7bb17"
      },
      "source": [
        "sentence = \"Do apples taste better than oranges in general?\"\r\n",
        "infernce(sentence)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Original Question ::\n",
            "Do apples taste better than oranges in general?\n",
            "\n",
            "\n",
            "Paraphrased Questions :: \n",
            "0: Is apples or oranges better than cucumbers?\n",
            "1: Why are apples considered to be better than oranges?\n",
            "2: Why would apples taste better than oranges?\n",
            "3: How good are apples or oranges?\n",
            "4: Does apples taste better than apples?\n",
            "5: Why do apples like oranges better than apples?\n",
            "6: Does apples taste better than oranges?\n",
            "7: Do apples taste better than oranges?\n",
            "8: Do apples or oranges taste good?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92e_P-YFzgTi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}